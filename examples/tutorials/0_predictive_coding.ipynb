{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 0: predictive coding via **pcax**\n",
    "In this notebook, you will learn how to use pcax to build arbitrary predictive coding networks. We will focus on a fully connected network trained on MNIST: the \"hello world\" of neural networks and deep learning.\n",
    "The current version of the library is 0.5.0, which had some minor but impactful changes compared to v0.3.0. Be sure to use the right version of pcax, or you'll run into a bunch of syntax errors!\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Importing dependencies\n",
    "pcax is based on JAX and combines the ideas behind [equinox](https://github.com/patrick-kidger/equinox) and [objax](https://github.com/google/objax), two deep-learning libraries built on top of JAX. In particular, *equinox* is currently a dependency as we use the `nn` modules provided by it.\n",
    "\n",
    "\n",
    "The library is divided in three modules:\n",
    "- *core*: defines the basic building blocks of pcax, unrelated to predictive coding itself.\n",
    "- *pc*: here lies all the predictive coding implementation, which will probably keep changing and being updated as more discoveries are made in the field.\n",
    "- *nn*: simply contains the typical layers you could expect from a deep learning library, which are currently built as a wrap around *equinox* layers.\n",
    "- *utils*: various tools to ease the development of complex applications by providing shurtcuts for commond implementation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "import jax\n",
    "import optax\n",
    "\n",
    "# pcax\n",
    "\n",
    "# Importing pcax is equivalent to importing pcax.pc\n",
    "# which contains the functionalities to build a predictive coding network\n",
    "import pcax as px\n",
    "\n",
    "# A filter is core object of pcax. It is used to filter which parameters in a network\n",
    "# should undergo a specific JAX transformation (e.g. jax.grad, jax.jit, etc.).\n",
    "# Consequently, despite being a core object, pcax offers a shortuct to use it: pcax.f.\n",
    "\n",
    "# pcax.nn contains the neural network modules (e.g. Conv2d, Linear, etc.)\n",
    "# at the moment only Linear is implemented, but more coming soon!\n",
    "import pcax.nn as nn\n",
    "\n",
    "# pcax.utils contains some useful utilities to train and use the network\n",
    "import pcax.utils as pxu\n",
    "\n",
    "# finally we can import the library's core which is simply a wrapper around JAX,\n",
    "# unrelated to predictive coding. useful for some advanced configurations.\n",
    "# We will not use it in this tutorial. \n",
    "# import pcax.core as pxc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will also use the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "import timeit\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default JAX will use all the availble memory on the target GPU device.\n",
    "# This can be beneficial for performance but can also result in a device being completely\n",
    "# unsuable by other people. The following flag disables this behaviour.\n",
    "# PLEASE REMEMBER TO ALWAYS SET THIS FLAG when working on shared machines.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining a Model\n",
    "Defining a basic pcax model is very straightforward: simply interpone in the forward call a `px.Node` between any two `nn.Layer`s (e.g. linear layers followed by an activation function). To do so first define them in the `__init__`, no arguments are required for basic usage!\n",
    "\n",
    "In the `__init__`, we also define the activation function we are going to use and the specify we do not want to update the `x` of the last node, since it will contain the label we want our network to learn. It is, in fact, common practice to freeze the target nodes during training. Note this model does not directly support inference on the inputs as there is no node to store the input `x` of the forward call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model is defined in a way that is similar to PyTorch. In particular:\n",
    "# - we inherit from px.EnergyModule;\n",
    "# - we define the activation functions, nodes, and layers in the __init__ method;\n",
    "# - we define the forward pass in the __call__ method. By default, the forward pass will also be used to\n",
    "#   initialize the network as it will be described later.\n",
    "\n",
    "class Model(px.EnergyModule):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 output_dim: int,\n",
    "                 nm_layers: int,\n",
    "                 act_fn: Callable[[jax.Array], jax.Array]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.act_fn = act_fn\n",
    "\n",
    "        # This is quite standard. We define the layers and nodes (one node to follow each layer).\n",
    "        self.layers = [nn.Linear(input_dim, hidden_dim)] + [\n",
    "            nn.Linear(hidden_dim, hidden_dim) for _ in range(nm_layers - 1)\n",
    "        ] + [nn.Linear(hidden_dim, output_dim)]\n",
    "\n",
    "        self.nodes = [px.Node() for _ in range(nm_layers + 1)]\n",
    "\n",
    "        # We normally use the x of the last layer as the target, therefore we don't want to update it\n",
    "        # during training.\n",
    "        self.nodes[-1].x.frozen = True\n",
    "\n",
    "    # Here things are a bit different. __call__ accepts an optional target t (used during training),\n",
    "    # which is used to set the value of the last node to the target label.\n",
    "    def __call__(self,\n",
    "                 x: jax.Array,\n",
    "                 t: Optional[jax.Array] = None):\n",
    "\n",
    "        # !!! IMPORTANT !!!\n",
    "        # Each (pc) layer contains a cache the stores the important intermediate values computed in the forward pass.\n",
    "        # By default, these are the incoming activation 'u', the node value 'x' and the energy 'e'.\n",
    "        # You can access them by using the [] operator, e.g., self.pc[\"x\"].\n",
    "\n",
    "        # We forward x through the network, clamping the last node to the target label.\n",
    "        # We use the activation function defined in the __init__ method for all the layers but the last one.\n",
    "        # Notice how the input x is passed directly to the first layer before being saved to any node:\n",
    "        # this PCN works only in forward mode, therefore we don't need to save the input.\n",
    "        # Finally, note that the syntax `node(x)[\"x\"]` is, by default, a shortcut for the following:\n",
    "        # \n",
    "        # node[\"u\"] = x\n",
    "        # if node.is_init:\n",
    "        #     node[\"x\"] = node[\"u\"]\n",
    "        # return node[\"x\"]\n",
    "        #\n",
    "        # which means that, by default, the forward pass is also used to initialize the node values.\n",
    "        for node, layer in zip(self.nodes[:-1], self.layers[:-1]):\n",
    "            x = node(self.act_fn(layer(x)))[\"x\"]\n",
    "\n",
    "        x = self.nodes[-1](self.layers[-1](x))[\"x\"]\n",
    "\n",
    "        # If the have a target we use it to clamp the last node.\n",
    "        if t is not None:\n",
    "            self.nodes[-1][\"x\"] = t\n",
    "\n",
    "        # The output of the network is the activation received by the last layer (since its x is clamped to the label).\n",
    "        return self.nodes[-1][\"u\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the training parameters we are going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\": 256,\n",
    "    \"x_learning_rate\": 0.01,\n",
    "    \"w_learning_rate\": 1e-3,\n",
    "    \"num_epochs\": 4,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"input_dim\": 28 * 28,\n",
    "    \"output_dim\": 10,\n",
    "    \"seed\": 0,\n",
    "    \"T\": 4,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the dataloaders we'll need to train and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is all standard and uses PyTorch's datasets and dataloaders. We are assuming cuda is available to set the dataloaders' parameters.\n",
    "\n",
    "# We'll train with the standard pc energy function, that is, the sum of the squared differences between the node values and the target.\n",
    "# Therefore, we need to convert the targets to one-hot vectors.\n",
    "def one_hot(t, k):\n",
    "    return np.array(t[:, None] == np.arange(k), dtype=np.float32)\n",
    "\n",
    "\n",
    "# Used to convert the square 0-255 images to 0-1 float vectors.\n",
    "class FlattenAndCast:\n",
    "    def __call__(self, pic):\n",
    "        return np.ravel(np.array(pic, dtype=np.float32) / 255.0)\n",
    "\n",
    "\n",
    "train_dataset = MNIST(\n",
    "    \"/tmp/mnist/\",\n",
    "    transform=FlattenAndCast(),\n",
    "    download=True,\n",
    "    train=True,\n",
    ")\n",
    "train_dataloader = pxu.data.TorchDataloader(\n",
    "    train_dataset,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = MNIST(\n",
    "    \"/tmp/mnist/\",\n",
    "    transform=FlattenAndCast(),\n",
    "    download=True,\n",
    "    train=False,\n",
    ")\n",
    "test_dataloader = pxu.data.TorchDataloader(\n",
    "    test_dataset,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Where the fun begins\n",
    "As mentioned, pcax is based on JAX, which is a functional framework. Consequently, in order to offer a simple object oriented interface to it, there are some compromises to be made and strict patterns to follow. In particular, JAX requires to keep track of all the tensors involved in a computation. pcax does this by tracking all the `Params` contained in any `Module` passed to a function **as a keyword argument**. So be careful when defining a function... but we'll go into more details later.\n",
    "\n",
    "The way JAX works is by transforming simply functions to achieve complex behaviours. Let's see what this means!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1: Defining a function\n",
    "Let's see how a function involving a `px.Module` (`px.EnergyModule` is a `px.Module`) is defined. We'll proceed step by step (so only the last version is the correct one).\n",
    "\n",
    "Consider the function `predict(x, t)` which simply calls the forward pass of the model (we also include the optional target `t` as well since it is needed during training).\n",
    "In normal python, `predict` should look similar to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x, t = None):\n",
    "    return model(x, t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However**, since this function uses a `px.Module` (i.e., `model`), we need to treat it differently and explicitely pass it as a keyword argument. There's not much too add, it needs to be done :P\n",
    "So `predict` should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '*' means that all the following arguments must be passed by name.\n",
    "# It is standard Python syntax and it can be omitted if you remember to pass the arguments by name.\n",
    "# However, defining the function in this way makes it easier to use avoid this simple mistake.\n",
    "def predict(x, *, t = None, model = None):\n",
    "    return model(x, t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we are not done. There's one more convention that need to be followed, and it's the difference between static and dynamic parameters (another key JAX concept). In general, you can think at it in this way: a tensor (even if it contains a single number) is a dynamic value, anything else is static. In details, a dynamic parameter is a parameter that *does not* alter the flow of execution of a program, but only its output. Think about the following function:\n",
    "\n",
    "```python\n",
    "def op(a: float, b: float, op: str):\n",
    "    if op == '+':\n",
    "        return a + b\n",
    "    elif op == '-':\n",
    "        return a - b\n",
    "```\n",
    "\n",
    "Here, different `a` and `b` values will produce different results but the required computation dependes exclusively on `op` (given that the type of `a` and `b` is fixed). Thus, `a` and `b` are dynamic parameters, `op` is static.\n",
    "This, however, means that we cannot use the value of `a` or `b` to alter the flow of the function. For example, the following is not valid:\n",
    "\n",
    "```python\n",
    "def op(a: float, b: float, op: str):\n",
    "    if op == '+':\n",
    "        c = a + b\n",
    "    elif op == '-':\n",
    "        c = a - b\n",
    "\n",
    "    if c < 0:\n",
    "        return c + 1\n",
    "    else:\n",
    "        return c + 2\n",
    "```\n",
    "Here we condition on `c` which, being a product of dynamic parameters, is a dynamic parameter as well. This will result in a compilation error.\n",
    "There are primitives that allow us to dynamically execute different pieces of code based on dynamic values and we will see them in another tutorial.\n",
    "To conclude this section, we always have to decide if a parameter is static (i.e., we want to use it as a *flag* to compile the same code multiple times with different behaviours) or dynamic (it is an actual parameter of the function we define). What happens is that static parameters are hardcoded into the function when it is compiled, and using different static values for the same parameter will result in a recompilation of the function.\n",
    "\n",
    "pcax follows the following convention: positional arguments are dynamic, keyword arguments are static. Thus, we would have to do the following to call `op`: `op(1.0, 2.0, op=\"+\")`. Note that this is necessary only for JAX-transformed functions (we'll look at them in a moment).\n",
    "\n",
    "All of this means that `predict` should be defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again note that '*' is not necessary but it makes it easier to avoid mistakes.\n",
    "def predict(x, t = None, *, model = None):\n",
    "    return model(x, t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it is ready to be used by pcax!\n",
    "\n",
    "The next step is **vectorization** (used to achieve **batching** in this case). JAX (and therefore pcax) defines each computation on a tensor assuming it contains a single sample (that is, there is no batch dimension as it happens, for instance, in Pytorch). Therefore, in the function above, `x` is supposed to be an array with shape [784,] (e.g., a flattened MNIST image) and `t` an array with shape [10,] (the corresponding one-hot encoded label). If we want `predict` to be able to work on batched input (i.e., `x` with shape [n, 784]), we need to *vectorize* the function such that the `predict` computation will be repeated along the batch dimension. In JAX, this is achieved using `vmap`. In pcax, given its inspiration from objax, using `Vectorize` (this may change in the future as the library evolves past its original objax-like formulation).\n",
    "\n",
    "`pcax.utils` offers a list of functions and decorators that can be used to easily apply these transformations to any function. Since we have already imported it as `pxu`, we can \"batch\" `predict` by adding the following decorator to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxu.vectorize(px.f(px.NodeParam, with_cache=True), in_axis=(0, 0))\n",
    "def predict(x: jax.Array, t: Optional[jax.Array] = None, *, model):\n",
    "    return model(x, t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is quite a lot to unpack:\n",
    "- first of all, now predict can be exclusively be used with batched input, which means that both `x` and `t` must share the same size for dimension `0`.\n",
    "- `pxu.vectorize` is a thin wrapper around `pxc.Vectorize`. The first argument specifies which parameters of the models passed to the function we want to vectorize using `px.f`. As of now, for predictive coding, these are, always and only, the node values, which, in pcax, are identified as `px.NodeParam`. In fact, each data sample should have its own exclusive set of node values (on the contrary, weight values are shared between different batch samples and we do not want to have them vectorized). `with_cache` specifies that you want to capture not only the node values, but also their cached transformations (this for example includes the activation `u` produced by each layer and received by each node). Again, as of now, you will probably, always and only, have to specify it when using vectorize to batch a function.\n",
    "- `in_axis` specifies how to treat the *positional* arguments of the function (note how there are only two values in the tuple despite the function taking three: model, being a keyword, and thus static, argument, is automatically ignored). `0` means that the function will be vectorized over the first dimension of the corresponding argument while passing `None` means that the argument will be ignored by the vectorization and passed as it is to the function. In the case of `predict`, both `x` and `t` will be batches of sample data with shape [batch_dim, ...] so we pass `0` for both of them.\n",
    "- by default `pcu.vectorize` expects a single output vectorized along dimension `0`, which is exactly what we have here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at the definition of `px.f` for instructions and tips on how to use it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go to the next function we need: the loss function.  \n",
    "Similarly to how in Pytorch you compute the gradients from the loss by calling `loss.backward()`, here we compute the gradients by transforming the loss function such that is also output the gradients.\n",
    "\n",
    "In predictive coding, the standard loss function simply computes and returns the model's energy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxu.vectorize(px.f(px.NodeParam, with_cache=True), in_axis=(0, 0), out_axis=(\"sum\",))\n",
    "def loss(x: jax.Array, t: Optional[jax.Array] = None, *, model):\n",
    "    model(x)\n",
    "    return model.energy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe a few things:\n",
    "- we pass `t` (the target label) and compute `y` (the output of the model) as it's standard practice, however, here, they are not strictly necessary: we don't need the model's output to compute the error (as it is used already when computing the energy) and we assume that the target has already been set to the `x` of the last node when we initialized the node values (so, again, it is already included in the energy computation).\n",
    "- the first two parameters of `px.vectorize` are the same of `predict`, however here we add a modifier for the output (notice that, even if we have a single output, `out_axis` is specified using a tuple). We use `\"sum\"` to specify that the output of the function should be the summed over the batch dimension. We do this because, as it happens in Pytorch, the loss must be a single floating point value (we don't use the mean since we want the total energy coming from each error node, not their average).\n",
    "- `model.energy()` computes and caches the energy value for each node in `model`, returning their sum."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to define two different \"backward\" functions, one for the *x step* (in which we update the value nodes) and one for the *w step* (in which we update the weights). They are identical except for the fact that they compute gradients with respect of different elements.  \n",
    "`pxu.grad_and_values` transform a function such that is outputs the gradients with respect to the specified variables. Note how the filter used in either case does not specify `with_cache=True` as we do not want (and it would not make much sense) to compute the gradient with respect of cached intermediate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x computes the gradients with respect to the node values that are not frozen.\n",
    "train_x = pxu.grad_and_values(\n",
    "    px.f(px.NodeParam)(frozen=False), # px.f.__call__(**kawrgs) is used to apply a filter, selecting all the variables with the specified properties\n",
    ")(loss)\n",
    "\n",
    "# train_w computes the gradients with respect to the weights.\n",
    "train_w = pxu.grad_and_values(\n",
    "    px.f(px.LayerParam)\n",
    ")(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This covers almost everything you need to do about defining a function the operates on one (or more) `px.Module`. We can now define a model. This would normally be done inside the `if __name__ == \"__main__\"`, but to provide a more linear tutorial we introduce it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(28 * 28, params[\"hidden_dim\"], 10, 2, jax.nn.tanh)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2: Optimizers\n",
    "In pcax (as it is inspired from Objax), optimizers are `px.Module`s as well. Consequentely, they need to be treated similarly to how we used `model`. In particular, they must be passaed as keywords arguments to any function that uses them.\n",
    "pcax offers a single `px.Optim` class that allows `px.Module`s to interact with `optax` optimizers (the most common JAX library for optimizers). As you can see from the following example, you simply have to specify which `optax` optimizer to use for which subset of the `px.Module` variables. In this case, similarly to how we defined the two loss functions, we defined an *x optimizer* and a *w optimizer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy run to init the optimizer parameters\n",
    "with pxu.train(model, np.zeros((params[\"batch_size\"], 28 * 28)), None):\n",
    "    optim_x = pxu.Optim(\n",
    "        optax.sgd(params[\"x_learning_rate\"]),\n",
    "        model.parameters().filter(px.f(px.NodeParam)(frozen=False))\n",
    "    )\n",
    "    optim_w = pxu.Optim(\n",
    "        optax.adam(params[\"w_learning_rate\"]),\n",
    "        model.parameters().filter(px.f(px.LayerParam)),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may ask yourself what is first line for?\n",
    "\n",
    "It may be the case that your optimizers defines some parameters that are linked to the individual trainable parameter inside your model. For instance, you could choose to use `optax.adam` for `optim_x` (not recommended, for all we know you should stick with stateless optimizators for the value nodes). In this case, the optimizer needs to instantiate the `adam` parameters and, thus, it requires to know the shape of all the `px.NodeParams`s inside your model. However, if we do not run the model at least once, all the node values will be empty, since they are lazily created. Furthermore, the shape of the value nodes depends on the batch size (since, remember, we want different value nodes for each different input), so we need to perform a dummy run on a dummmy input with the same shape of the samples we are gonna train on to correctly initialize all the parameters inside the model. Only then we can safely create the two optimizers. (Note that this requires `batch_size` to be constant throught the program. To guarantee this, by default, the dataloaders have `drop_last=True`. Unless you know what you're doing, do not attempt to modify that.)\n",
    "\n",
    "It actually doesn't really matter what you pass as `x` and `t`, but only that their shape matches the one of the true input batches (but in this way you can see how you can pass `None` as the target of the predict function).\n",
    "\n",
    "`px.train` (and `px.eval`) does exactly this: under the hood it behaves like the `predict` function we defined previously (if you pass an arguments other than `model`). **NOTE**: This is one of the few, if not the only, \"hidden behaviours\" of pcax: `px.train` (and `px.eval`) calls a batched version of `model.__call__` if you provide any arguments to it. In order to customise its batching behaviour `px.train` accepts all the kwargs you would normally pass to `pxu.vectorize` (all the other args will be passed to `model.__call__`).\n",
    "\n",
    "In addition to this, `px.train`/`px.eval` set the internal status flag of the model such that either `is_train`/`is_eval` returns `True`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already hinted, it is important to know that `px.Module` uses an internal caching system to store intermediate values for later computations (such as a layer's activation `u`, necessary to compute a node's energy). If you need to run multiple forward passes on the same input (like during training), you have to enclose each one of them into a `px.step` context manager. This ensures that the cache is cleared and the next forward pass with compute new updated values instead of reusing previous ones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3: Training the model\n",
    "We are almost done. We have all the ingredients to train the model, we just need to assemble them in the training and evaluation functions. There's only one core concept to introduce: *jitting*. *Just In Time* compilation allows the program to get compiled and optimized (it's the reason we have so many constraints on how code should be written). We jit the computations executed on a single batch as we want to compile as much code as possible, and `train_on_batch` represents the single largest repeated operation occurring during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxu.jit()\n",
    "def train_on_batch(x, y, *, model, optim_w, optim_x):\n",
    "    # We are working on the input x, so we initialise the internal nodes with it.\n",
    "    with pxu.train(model, x, y) as (y_hat,):\n",
    "        for i in range(params[\"T\"]):\n",
    "            # Each forward pass caches the intermediate values (such as activations and energies), so we can use them to compute the gradients.\n",
    "            # px.step takes care of managing the cache.\n",
    "            with pxu.step(model):\n",
    "                g, (v,) = train_x(x, y, model=model)\n",
    "                optim_x(g)\n",
    "\n",
    "        with pxu.step(model):\n",
    "            g, (v,) = train_w(x, y, model=model)\n",
    "            optim_w(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few remarks:\n",
    "- this is the standard pc training procedure:\n",
    "    - we initialize the node values using `px.train` (which by default uses forward initialization)\n",
    "    - we repeat the *x-update* step for `T` times and then we perform a single *w-update*.\n",
    "- `optim_*(g)` updates the node values/weights of the model, therefore all the computed cached values must be cleared. That's why we enclose each trainig operation in a `px.step`.\n",
    "- you can see that the values returned by `pxu.train` (and `predict`) and `train_*` (the first value returned by `train_*` are the computed gradients, and then there are the actual return values of the function) are all tuples, even if a single value is acutally returned by the orginal functions (before being transformed). Just something to keep in mind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the remaining functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxu.jit()\n",
    "def evaluate(x, y, *, model):\n",
    "    # As in train_on_batch, we initialise the internal nodes with the input x. By doing so we also get the model's output y_hat.\n",
    "    with pxu.eval(model, x, y) as (y_hat,):\n",
    "        return (y_hat.argmax(-1) == y.argmax(-1)).mean()\n",
    "\n",
    "\n",
    "def epoch(dl):\n",
    "    for batch in dl:\n",
    "        x, y = batch\n",
    "        y = one_hot(y, 10)\n",
    "\n",
    "        train_on_batch(x, y, model=model, optim_w=optim_w, optim_x=optim_x)\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test(dl):\n",
    "    accuracies = []\n",
    "    for batch in dl:\n",
    "        x, y = batch\n",
    "        y = one_hot(y, 10)\n",
    "\n",
    "        accuracies.append(evaluate(x, y, model=model))\n",
    "\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the main body:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling + Epoch 1 took 2.5920134061016142 seconds\n",
      "An Epoch takes on average 1.1743265149998479 seconds\n",
      "Final Accuracy: 0.9599359\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    t = timeit.timeit(lambda: epoch(train_dataloader), number=1)\n",
    "    print(\"Compiling + Epoch 1 took\", t, \"seconds\")\n",
    "\n",
    "    # Time of an epoch (without jitting)\n",
    "    t = timeit.timeit(lambda: epoch(train_dataloader), number=params[\"num_epochs\"]) / params[\"num_epochs\"]\n",
    "    print(\"An Epoch takes on average\", t, \"seconds\")\n",
    "\n",
    "    print(\"Final Accuracy:\", test(test_dataloader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is all right, you should get a final accuracy of ~96%. On pssr2, you should also get a time per epoch of ~1.20. The training time is actually heavily bottlenecked by the data transfer between CPU and GPU, that's why we are using 8 workers in each dataloader. So depending on your configuration the final speed my change.\n",
    "\n",
    "**However**, there is one more step that we can do to improve performance: using `snapshots`. JAX jitting works by keeping track of all the dynamic and static arguments you pass to a jitted function: if the type/shape of a dynamic argument or value of a static one changes the function needs to be recompiled. Of course, this is the correct intended behaviour. However, checking for these changes is computationally expensive. pcax let the user specifies that some static arguments (e.g., the modules we use) are not gonna change between calls of the same function (we are not magically gonna add new parameters to our model while its training...) by creating a snapshot of them at a specific state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = train_on_batch.snapshot(model=model, optim_w=optim_w, optim_x=optim_x)\n",
    "test_fn = evaluate.snapshot(model=model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these functions inside `epoch` and `test` and speed everything up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(dl, train_fn):\n",
    "    for batch in dl:\n",
    "        x, y = batch\n",
    "        y = one_hot(y, 10)\n",
    "\n",
    "        train_fn(x, y)\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test(dl, test_fn):\n",
    "    accuracies = []\n",
    "    for batch in dl:\n",
    "        x, y = batch\n",
    "        y = one_hot(y, 10)\n",
    "\n",
    "        accuracies.append(test_fn(x, y))\n",
    "\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want a fair training comparison, let's reset the model parameters before training it again. This can be done using `px.move`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"(Model).layers.(SequenceKey(idx=0), 'nn.(FlattenedIndexKey(key=0),)')\": LayerParam(Array([[ 0.02440736, -0.01989807, -0.02523823, ...,  0.00115597,\n",
       "         -0.00592463, -0.02165587],\n",
       "        [-0.00646974, -0.01067329,  0.00959908, ..., -0.01409405,\n",
       "          0.03148658, -0.00121623],\n",
       "        [-0.03293773,  0.01134378, -0.02049655, ..., -0.01716941,\n",
       "          0.00946545,  0.00281346],\n",
       "        ...,\n",
       "        [-0.0266142 , -0.01326478, -0.0049628 , ...,  0.02577982,\n",
       "          0.01908015,  0.02463696],\n",
       "        [ 0.01897506,  0.0019553 , -0.0295014 , ..., -0.00663123,\n",
       "         -0.02676079,  0.00803781],\n",
       "        [ 0.01315161,  0.03167168, -0.00363353, ...,  0.0094413 ,\n",
       "         -0.02631798,  0.03374758]], dtype=float32), reduce=reduce_none),\n",
       " \"(Model).layers.(SequenceKey(idx=0), 'nn.(FlattenedIndexKey(key=1),)')\": LayerParam(Array([-0.00415224, -0.00198118,  0.0173532 , -0.00469784,  0.00166869,\n",
       "         0.00224254, -0.02744545,  0.01556099, -0.00154132,  0.00123018,\n",
       "        -0.02936435,  0.01686131,  0.00068845, -0.01243156, -0.00141367,\n",
       "         0.01000284,  0.01844218,  0.01372359, -0.03020925, -0.02479543,\n",
       "         0.01338834, -0.01892889, -0.01314371, -0.0144968 ,  0.0098369 ,\n",
       "        -0.03279685,  0.02086507, -0.00031115,  0.00240749,  0.03538868,\n",
       "        -0.00013452,  0.00262055, -0.00366326,  0.00131243,  0.01033748,\n",
       "         0.01116097,  0.00982281, -0.0252365 ,  0.02942479, -0.01670223,\n",
       "         0.01303016,  0.01347722, -0.02197532,  0.02781434, -0.03007878,\n",
       "         0.02800733,  0.01236656,  0.02268129,  0.01325501,  0.02688036,\n",
       "        -0.00835518, -0.02384405,  0.00936967,  0.03394109, -0.03172595,\n",
       "        -0.00537618,  0.00757796,  0.00405462,  0.0317986 ,  0.02262224,\n",
       "         0.01259205, -0.01228555,  0.03251017, -0.01629781,  0.03224493,\n",
       "        -0.02767812, -0.03499879, -0.03008008, -0.00646622,  0.02515546,\n",
       "        -0.01041453, -0.01388873,  0.02136701,  0.03295087, -0.00244699,\n",
       "        -0.02247444, -0.00756943,  0.01783916,  0.03218776,  0.01892364,\n",
       "         0.00242886, -0.0289171 ,  0.02368777,  0.00046088,  0.00682482,\n",
       "        -0.03103576,  0.03174591, -0.01757765,  0.01217612,  0.02533046,\n",
       "        -0.02397623,  0.01593527,  0.00284549, -0.03356744, -0.00270459,\n",
       "        -0.01483225,  0.01505532, -0.01168101, -0.03209809,  0.02420219,\n",
       "        -0.01262094,  0.02881649,  0.00371207,  0.02145828, -0.01155898,\n",
       "         0.03556326, -0.00256228, -0.0138732 , -0.02519958,  0.01946224,\n",
       "         0.00741251,  0.01717444, -0.03131684, -0.00103837,  0.00476764,\n",
       "         0.03510636, -0.02109234,  0.02757634, -0.02764627, -0.02353409,\n",
       "        -0.01198145,  0.03418062, -0.01744247,  0.02946577, -0.03174467,\n",
       "         0.00486857,  0.01438992,  0.02980665], dtype=float32), reduce=reduce_none),\n",
       " \"(Model).layers.(SequenceKey(idx=1), 'nn.(FlattenedIndexKey(key=0),)')\": LayerParam(Array([[ 0.06570164,  0.00857758, -0.01052716, ..., -0.05924709,\n",
       "          0.01804237,  0.05629059],\n",
       "        [-0.08751047,  0.04212946,  0.05001714, ...,  0.04911821,\n",
       "          0.0520499 , -0.02748546],\n",
       "        [-0.03756481,  0.05537125,  0.05108202, ...,  0.06290225,\n",
       "          0.04735842, -0.02591955],\n",
       "        ...,\n",
       "        [-0.02770256, -0.08095657,  0.08029304, ...,  0.05414722,\n",
       "         -0.06270189,  0.01757106],\n",
       "        [ 0.04184631,  0.06692377, -0.00803631, ...,  0.03862613,\n",
       "          0.04266233, -0.04392399],\n",
       "        [-0.05616078, -0.07294711, -0.04609984, ...,  0.0462792 ,\n",
       "         -0.07824242, -0.06202539]], dtype=float32), reduce=reduce_none),\n",
       " \"(Model).layers.(SequenceKey(idx=1), 'nn.(FlattenedIndexKey(key=1),)')\": LayerParam(Array([-0.08573245,  0.07395288,  0.08389794,  0.05772255, -0.02728435,\n",
       "        -0.0675665 ,  0.00809217, -0.04966081,  0.01701652,  0.05405886,\n",
       "        -0.04774667, -0.02461319, -0.01781058, -0.00330956, -0.08529515,\n",
       "         0.05549749, -0.06992821,  0.04288533, -0.04376185, -0.05542521,\n",
       "        -0.08700709,  0.08411499,  0.07301999, -0.02435476,  0.03899879,\n",
       "        -0.01906235, -0.00289749, -0.06022367,  0.05451781,  0.00676255,\n",
       "        -0.0636924 ,  0.08692751,  0.07517733, -0.01179874, -0.06401832,\n",
       "         0.02807149,  0.03532489,  0.06702446, -0.00114781, -0.0253571 ,\n",
       "         0.02056977, -0.08255132,  0.00303761,  0.00594933,  0.02485979,\n",
       "         0.07519033,  0.02142322,  0.00041329, -0.06782165,  0.04653739,\n",
       "        -0.00958059,  0.00365959, -0.05474869,  0.07036749,  0.04864814,\n",
       "        -0.07390707,  0.04283587, -0.05577994,  0.06599354, -0.06305586,\n",
       "         0.07876179,  0.05174159,  0.07694574,  0.07552049,  0.04313766,\n",
       "         0.08220663,  0.06975476,  0.07183688,  0.07010344, -0.03896177,\n",
       "        -0.00694177, -0.06365   ,  0.01774304, -0.07836589, -0.07597327,\n",
       "         0.04671947, -0.06901748, -0.06174397,  0.04604701, -0.07905153,\n",
       "        -0.08272305, -0.08196251,  0.05756035, -0.01175828, -0.02338048,\n",
       "         0.02270866, -0.03047683, -0.03538491,  0.07604212,  0.01674829,\n",
       "        -0.01814251, -0.0183885 ,  0.00991283,  0.07286324,  0.01530824,\n",
       "         0.02616753, -0.07902207,  0.02214031, -0.05516217,  0.08325691,\n",
       "        -0.0151092 , -0.00783657,  0.02457522,  0.08506159, -0.02848016,\n",
       "         0.0540724 ,  0.00841289,  0.00651263, -0.02032428, -0.00146233,\n",
       "         0.06808842, -0.05628507,  0.01805931, -0.05458308,  0.0505173 ,\n",
       "        -0.07701308, -0.05399488, -0.02465238, -0.02886288,  0.07664729,\n",
       "        -0.07346893,  0.0388866 , -0.06207723,  0.06941303, -0.06977832,\n",
       "         0.04231191,  0.04724226, -0.0701537 ], dtype=float32), reduce=reduce_none),\n",
       " \"(Model).layers.(SequenceKey(idx=2), 'nn.(FlattenedIndexKey(key=0),)')\": LayerParam(Array([[ 0.08220357, -0.03461396, -0.04564089, ..., -0.03589173,\n",
       "         -0.0017531 ,  0.00018842],\n",
       "        [ 0.04304679, -0.05188068,  0.00253003, ..., -0.06116112,\n",
       "         -0.07417144,  0.06760556],\n",
       "        [ 0.05799132, -0.00799302, -0.08794431, ..., -0.06881931,\n",
       "          0.03621251,  0.02723829],\n",
       "        ...,\n",
       "        [ 0.07381528,  0.00255895, -0.01347279, ..., -0.08701745,\n",
       "          0.05874247, -0.01164351],\n",
       "        [-0.08471692,  0.01858211, -0.03159317, ...,  0.0506741 ,\n",
       "         -0.02802127,  0.07264448],\n",
       "        [-0.05546871,  0.06748983, -0.03881046, ..., -0.0590415 ,\n",
       "          0.01307768,  0.02988838]], dtype=float32), reduce=reduce_none),\n",
       " \"(Model).layers.(SequenceKey(idx=2), 'nn.(FlattenedIndexKey(key=1),)')\": LayerParam(Array([-0.03102792,  0.03071068, -0.00865768,  0.00378884,  0.00268366,\n",
       "         0.07741529, -0.02538416, -0.02922009, -0.01638146, -0.00430486],      dtype=float32), reduce=reduce_none)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a newly randomly initialised model and copy its layer parameters to the trained model.\n",
    "px.move(\n",
    "    Model(28 * 28, params[\"hidden_dim\"], 10, 2, jax.nn.tanh).parameters().filter(px.f(px.LayerParam)),\n",
    "    model.parameters().filter(px.f(px.LayerParam))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main body now looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling + Epoch 1 took 1.6082754489034414 seconds\n",
      "An Epoch takes on average 0.7335431027458981 seconds\n",
      "Final Accuracy: 0.958734\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    t = timeit.timeit(lambda: epoch(train_dataloader, train_fn), number=1)\n",
    "    print(\"Compiling + Epoch 1 took\", t, \"seconds\")\n",
    "\n",
    "    # Time of an epoch (without jitting)\n",
    "    t = timeit.timeit(lambda: epoch(train_dataloader, train_fn), number=params[\"num_epochs\"]) / params[\"num_epochs\"]\n",
    "    print(\"An Epoch takes on average\", t, \"seconds\")\n",
    "\n",
    "    print(\"Final Accuracy:\", test(test_dataloader, test_fn))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training time should be reduced by ~40%, to around ~0.7 seconds per epoch. The compiling happens again as we have to compile the newly created snapshot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74243c937e8313cc140bb53b53727c6bafb36702322378655a2001e9641b5eb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
