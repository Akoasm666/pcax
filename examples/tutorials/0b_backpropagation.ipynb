{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 0b: **backpropagation with pcax**\n",
    "In this notebook, you will learn how to use pcax to a network equivalent to the one in *Tutorial 0*, but trained using backpropagation.\n",
    "The general idea is to simply remove all the bits related to predictive coding, nothing more than that.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "import jax\n",
    "import optax\n",
    "\n",
    "# pcax\n",
    "\n",
    "# Importing pcax is equivalent to importing pcax.pc\n",
    "# which contains the functionalities to build a predictive coding network\n",
    "import pcax as px\n",
    "\n",
    "# A filter is core object of pcax. It is used to filter which parameters in a network\n",
    "# should undergo a specific JAX transformation (e.g. jax.grad, jax.jit, etc.).\n",
    "# Consequently, despite being a core object, pcax offers a shortuct to use it: pcax.f.\n",
    "\n",
    "# pcax.nn contains the neural network modules (e.g. Conv2d, Linear, etc.)\n",
    "# at the moment only Linear is implemented, but more coming soon!\n",
    "import pcax.nn as nn\n",
    "\n",
    "# pcax.utils contains some useful utilities to train and use the network\n",
    "import pcax.utils as pxu\n",
    "\n",
    "# finally we can import the library's core which is simply a wrapper around JAX,\n",
    "# unrelated to predictive coding. useful for some advanced configurations.\n",
    "# We will not use it in this tutorial. \n",
    "# import pcax.core as pxc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "import timeit\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step: removing all pc layers from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we use px.Module instead of px.EnergyModule as we don't need to compute the energy\n",
    "class Model(px.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 output_dim: int,\n",
    "                 nm_layers: int,\n",
    "                 act_fn: Callable[[jax.Array], jax.Array]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.act_fn = act_fn\n",
    "        \n",
    "        self.layers = [nn.Linear(input_dim, hidden_dim)] + [\n",
    "            nn.Linear(hidden_dim, hidden_dim) for _ in range(nm_layers - 1)\n",
    "        ] + [nn.Linear(hidden_dim, output_dim)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.act_fn(layer(x))\n",
    "\n",
    "        x = self.layers[-1](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step: we don't need pc related params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\": 256,\n",
    "    # \"x_learning_rate\": 0.01,\n",
    "    \"w_learning_rate\": 1e-3,\n",
    "    \"num_epochs\": 4,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"input_dim\": 28 * 28,\n",
    "    \"output_dim\": 10,\n",
    "    \"seed\": 0,\n",
    "    # \"T\": 4,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No changes in the data loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(t, k):\n",
    "    return np.array(t[:, None] == np.arange(k), dtype=np.float32)\n",
    "\n",
    "\n",
    "class FlattenAndCast:\n",
    "    def __call__(self, pic):\n",
    "        return np.ravel(np.array(pic, dtype=np.float32) / 255.0)\n",
    "\n",
    "\n",
    "train_dataset = MNIST(\n",
    "    \"/tmp/mnist/\",\n",
    "    transform=FlattenAndCast(),\n",
    "    download=True,\n",
    "    train=True,\n",
    ")\n",
    "train_dataloader = pxu.data.TorchDataloader(\n",
    "    train_dataset,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = MNIST(\n",
    "    \"/tmp/mnist/\",\n",
    "    transform=FlattenAndCast(),\n",
    "    download=True,\n",
    "    train=False,\n",
    ")\n",
    "test_dataloader = pxu.data.TorchDataloader(\n",
    "    test_dataset,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the predict function we don't need the target (it could be used to fix the last pc node which does not exit anymore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the change to `in_axis` and the absence of a filter\n",
    "@pxu.vectorize(in_axis=(0,))\n",
    "def predict(x: jax.Array, *, model = None):\n",
    "    return model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is slightly different, as now we use the target to compute the actual loss (as we normally do in backpropagation). We also only need one gradient function and a single optimizer (since we only optimize the weights) so we can define all together by merging transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxu.grad_and_values(px.f(px.LayerParam))\n",
    "@pxu.vectorize(in_axis=(0, 0), out_axis=(\"sum\", 0))\n",
    "def loss(x: jax.Array, t: jax.Array, *, model = None):\n",
    "    y = model(x)\n",
    "    # we also return y even if we will not use it, to show how it can be done. Note the change in out_axis.\n",
    "    return (jax.numpy.square(y - t)).sum(), y # it's MSE loss without the Mean, since in pc we use the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = loss # change of name to be consistent with the other tutorials\n",
    "model = Model(28 * 28, params[\"hidden_dim\"], 10, 2, jax.nn.tanh)\n",
    "\n",
    "\"\"\"\n",
    "There's actually no need to initialize the model before defining the optimizer,\n",
    "however it is good practice to do so as it may be necessary in the general case.\n",
    "\"\"\"\n",
    "with pxu.train(model): # we don't pass any arguments here, since we don't need to initialize anything\n",
    "    optim = pxu.Optim(\n",
    "        optax.adam(params[\"w_learning_rate\"]),\n",
    "        model.parameters().filter(px.f(px.LayerParam)),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and evaluation functions are very intuitive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxu.jit()\n",
    "def train_on_batch(x, y, *, model, optim):\n",
    "    \"\"\"\n",
    "    Again, we don't need to initialize any node so we don't need to pass any argument to 'pxu.train'.\n",
    "    If we need the predicted y_hat we can use the value returned by the 'train' function.\n",
    "    \"\"\"\n",
    "    with pxu.train(model):\n",
    "        g, (v, y_hat) = train(x, y, model=model)\n",
    "        optim(g)\n",
    "\n",
    "\n",
    "@pxu.jit()\n",
    "def evaluate(x, y, *, model):\n",
    "    # There are no nodes to initialize, but we can use px.eval to compute y_hat.\n",
    "    # An alternative way would be to use step with the batched predict function.\n",
    "    with pxu.eval(model, x) as (y_hat,):\n",
    "        return (y_hat.argmax(-1) == y.argmax(-1)).mean()\n",
    "\n",
    "    # Alternative version:\n",
    "    \n",
    "    # with px.step(model):\n",
    "    #     y_hat, = predict(x, model=model)\n",
    "    #     return (y_hat.argmax(-1) == y.argmax(-1)).mean()\n",
    "\n",
    "\n",
    "def epoch(dl, train_fn):\n",
    "    for batch in dl:\n",
    "        x, y = batch\n",
    "        y = one_hot(y, 10)\n",
    "\n",
    "        train_fn(x, y)\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test(dl, test_fn):\n",
    "    accuracies = []\n",
    "    for batch in dl:\n",
    "        x, y = batch\n",
    "        y = one_hot(y, 10)\n",
    "\n",
    "        accuracies.append(test_fn(x, y))\n",
    "\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling + Epoch 1 took 1.718640837003477 seconds\n",
      "An Epoch takes on average 0.7030591029906645 seconds\n",
      "Final Accuracy: 0.95863384\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_fn = train_on_batch.snapshot(model=model, optim=optim)\n",
    "    test_fn = evaluate.snapshot(model=model)\n",
    "\n",
    "    t = timeit.timeit(lambda: epoch(train_dataloader, train_fn), number=1)\n",
    "    print(\"Compiling + Epoch 1 took\", t, \"seconds\")\n",
    "\n",
    "    # Time of an epoch (without jitting)\n",
    "    t = timeit.timeit(lambda: epoch(train_dataloader, train_fn), number=params[\"num_epochs\"]) / params[\"num_epochs\"]\n",
    "    print(\"An Epoch takes on average\", t, \"seconds\")\n",
    "\n",
    "    print(\"Final Accuracy:\", test(test_dataloader, test_fn))\n",
    "\n",
    "    del train_dataloader\n",
    "    del test_dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is all right, you should get a final accuracy of ~96% and a training time of ~0.7 second per epoch. The training time is actually heavily bottlenecked by the data transfer between CPU and GPU, that's why we are using 8 workers in the dataloader. So depending on your configuration the final speed my change.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pcax')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4fd8fe68194e475a108d2c5b5ee1f820870a7a5127298df9ccb3dbbd47c3153"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
