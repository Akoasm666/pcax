{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tutorial 1: **Backward inference**\n",
                "In this notebook, you will learn how to use pcax to do backward inference, that is predicting the inputs given the output.\n",
                "Since the library is still in its early development, expect major syntax changes. You can keep coming back to this notebook to stay updated.\n",
                "\n",
                "Good luck!"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 0: Importing dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Switched to CUDA 11.7.\n"
                    ]
                }
            ],
            "source": [
                "!source switch-cuda 11.7\n",
                "\n",
                "# Core dependencies\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "import optax\n",
                "import numpy as np\n",
                "\n",
                "# pcax\n",
                "import pcax as px # same as import pcax.pc as px\n",
                "import pcax.nn as nn\n",
                "from pcax.core import _\n",
                "\n",
                "# Environment variables\n",
                "import os\n",
                "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Defining a Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Model(px.Module):\n",
                "    def __init__(self, input_dim, hidden_dim, output_dim) -> None:\n",
                "        super().__init__()\n",
                "\n",
                "        self.act_fn = jax.nn.tanh\n",
                "\n",
                "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
                "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
                "        self.linear3 = nn.Linear(hidden_dim, output_dim)\n",
                "\n",
                "        self.pc0 = px.Layer()\n",
                "        self.pc1 = px.Layer()\n",
                "        self.pc2 = px.Layer()\n",
                "        self.pc3 = px.Layer()\n",
                "\n",
                "        self.pc3.x.frozen = True\n",
                "\n",
                "    def __call__(self, x, t=None):\n",
                "        x = self.pc0(x)[\"x\"]\n",
                "        x = self.pc1(self.act_fn(self.linear1(x)))[\"x\"]\n",
                "        x = self.pc2(self.act_fn(self.linear2(x)))[\"x\"]\n",
                "        x = self.pc3(self.linear3(x))[\"x\"]\n",
                "\n",
                "        if t is not None:\n",
                "            self.pc3[\"x\"] = t\n",
                "\n",
                "        return self.pc3[\"u\"]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Defining the data\n",
                "\n",
                "This part is unrelated to pcax. The only highlight is noticing how JAX handles randomness by passing a key around. pcax stores the key internally and allows to access it via a `px.random.RandomKeyGenerator`. We will see later an example of its usage."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.datasets import make_moons\n",
                "\n",
                "def create_data(params, rkey):\n",
                "    rkey1, rkey2 = jax.random.split(rkey)\n",
                "    if params[\"dataset\"] == \"xor\":\n",
                "        x_dataset = jax.random.randint(rkey1, (params[\"num_batches\"], params[\"batch_size\"], 2), 0, 2)\n",
                "        y_dataset = ((jnp.sum(x_dataset, axis=-1) % 2)[:, :, None]  == jnp.arange(2)).astype(jnp.float32)\n",
                "        x_dataset = x_dataset.astype(jnp.float32) + jax.random.normal(rkey2, (params[\"num_batches\"], params[\"batch_size\"], 2)) * params[\"data_noise\"]\n",
                "    elif params[\"dataset\"] == \"two_moons\":\n",
                "        x_dataset, y_dataset = make_moons(n_samples=params[\"batch_size\"], shuffle=True, noise=params[\"data_noise\"], random_state=rkey[0].item())\n",
                "        y_dataset = np.stack([1- y_dataset, y_dataset], axis=1)\n",
                "        x_dataset, y_dataset = jnp.array(x_dataset), jnp.array(y_dataset).astype(jnp.float32)\n",
                "        x_dataset, y_dataset = jnp.expand_dims(x_dataset, axis=0), jnp.expand_dims(y_dataset, axis=0)\n",
                "    else:\n",
                "        raise NotImplementedError(f\"Dataset not implemented: {params['dataset']}\")\n",
                "    return x_dataset, y_dataset"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Backward inference\n",
                "Predictive coding allows us to perform backward inference by fixing the output nodes (i.e., the coordinates of a point) and predicting the input nodes (i.e., its label) that minimise the total energy of the network. To achieve so, we need to modify the inference phase. In fact, the training phase (as it always fixes both input and output nodes) does not differ from normal forward inference training."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The main problem to solve in backward inference is how to initialise the value nodes during inference: since we do not have any input value what should we propagate through the network to get the best performance? This is still an open question. In this tutorial we found that the average input node activation (i.e., the vector [0.5, 0.5]) seems to work quite well, however feel free to experiment around it. The network can be configure to use any constant, random noise, etc. as input value."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "params = {\n",
                "    \"dataset\": \"xor\",\n",
                "    \"data_noise\": 0.05,\n",
                "    \"num_epochs\": 128,\n",
                "    \"hidden_dim\": 16,\n",
                "    \"num_batches\": 128,\n",
                "    \"batch_size\": 64,\n",
                "    \"T\": 8,\n",
                "\n",
                "    \"optim_x_l2\": 0.001,\n",
                "    \"optim_x_lr\": 0.5,\n",
                "    \"optim_w_l2\": 0.01,\n",
                "    \"optim_w_lr\": 5e-3,\n",
                "    \"optim_w_momentum\": 0.99,\n",
                "    \"optim_w_nesterov\": True,\n",
                "\n",
                "    # For the training phase we forward the input values\n",
                "    \"init_train\": {\n",
                "        \"init_rand_weight\": 0,\n",
                "        \"init_forward_weight\": 1.0,\n",
                "        \"init_constant\": 0,\n",
                "        \"init_constant_weight\": 0,\n",
                "    },\n",
                "    # For the test phase we initialize the input values with a constant\n",
                "    # since we don't have any input values\n",
                "    \"init_test\": {\n",
                "        \"init_rand_weight\": 0,\n",
                "        \"init_forward_weight\": 0,\n",
                "        \"init_constant\": 0.5,\n",
                "        \"init_constant_weight\": 1.0,\n",
                "    },\n",
                "\n",
                "    \"logs_plot_kwargs\": {},\n",
                "}\n",
                "\n",
                "model = Model(2, params[\"hidden_dim\"], 2)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To avoid any dependencies between the batch_size and the learning rates we sum over the energies of each input sample and divide the `optim_w_lr` by the batch size."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "@px.vectorize(_(px.NodeVar), in_axis=(0,), out_axis=(\"sum\",))\n",
                "@px.bind(model)\n",
                "def loss(x):\n",
                "    y = model(x)\n",
                "    return model.energy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "with px.init_nodes(model, np.zeros((params[\"batch_size\"], 2)), None):\n",
                "    optim_x = px.Optim(\n",
                "        optax.chain(\n",
                "            optax.add_decayed_weights(weight_decay=params[\"optim_x_l2\"]),\n",
                "            optax.sgd(params[\"optim_x_lr\"])\n",
                "        ),\n",
                "        model.vars(_(px.NodeVar)(frozen=False))\n",
                "    )\n",
                "    optim_w = px.Optim(\n",
                "        optax.chain(\n",
                "            optax.add_decayed_weights(weight_decay=params[\"optim_w_l2\"]),\n",
                "            optax.sgd(params[\"optim_w_lr\"] / params[\"batch_size\"], momentum=params[\"optim_w_momentum\"], nesterov=params[\"optim_w_nesterov\"])\n",
                "        ),\n",
                "        model.vars(_(px.Parameter) - _(px.NodeVar)),\n",
                "    )"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`get_node_initiator` generates the value to pass to `px.init_nodes`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_node_initiator(params):\n",
                "    def node_initiator(input = None, shape = None, key = None):\n",
                "        if shape is None:\n",
                "            shape = input.shape\n",
                "        if input is None:\n",
                "            input = jnp.zeros(shape)\n",
                "        r = jax.random.normal(key, shape) * params[\"init_rand_weight\"]\n",
                "        d = input * params[\"init_forward_weight\"]\n",
                "        c = jnp.full(shape, params[\"init_constant\"]) * params[\"init_constant_weight\"]\n",
                "        return r + d + c\n",
                "    return node_initiator"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Differently from forward inference, we need to perform an iterative phase in order to retrieve the correct value from the input layer (our output). Notice how `y = model.pc0[\"x\"]`, which is the first layer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "@px.jit()\n",
                "@px.bind(model, optim_x)\n",
                "def free_test_on_batch(t, x, loss, T, init_node):\n",
                "    forward_value = init_node(t, t.shape, px.core.RKG())\n",
                "    with px.init_nodes(model, forward_value, x):\n",
                "        for i in range(T):\n",
                "            with px.init_cache(model):\n",
                "                # Notice how we pass in input model.pc0[\"x\"], in this way\n",
                "                # the energy of the first layer is 0 and the value nodes are\n",
                "                # influenced only by the following layers.\n",
                "                g, (l,) = px.gradvalues(\n",
                "                    _(px.NodeVar)(frozen=False),\n",
                "                )(loss)(model.pc0[\"x\"])\n",
                "                \n",
                "                optim_x(g)\n",
                "        \n",
                "        y = model.pc0[\"x\"]\n",
                "    \n",
                "    target_class = jnp.argmax(t, axis=1)\n",
                "    predicted_class = jnp.argmax(y, axis=1)\n",
                "    accuracy = jnp.mean(predicted_class == target_class)\n",
                "\n",
                "    return accuracy"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We use PPC to train the model, that is we update nodes and weights at the same time. This seems to improve performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "@px.jit()\n",
                "@px.bind(model, optim_x=optim_x, optim_w=optim_w)\n",
                "def train_on_batch(t, x, loss, T, init_node):\n",
                "    forward_value = init_node(t, t.shape, px.core.RKG())\n",
                "    with px.init_nodes(model, forward_value, x):\n",
                "        for i in range(T):\n",
                "            with px.init_cache(model):\n",
                "                g, (l,) = px.gradvalues(\n",
                "                    _(_(px.NodeVar)(frozen=False), _(px.Parameter) - _(px.NodeVar)),\n",
                "                )(loss)(forward_value)\n",
                "\n",
                "                optim_x(g)\n",
                "                optim_w(g)\n",
                "\n",
                "        y = model.pc0[\"x\"]\n",
                "\n",
                "    target_class = jnp.argmax(t, axis=1)\n",
                "    predicted_class = jnp.argmax(y, axis=1)\n",
                "    accuracy = jnp.mean(predicted_class == target_class)\n",
                "\n",
                "    return accuracy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "@px.bind(model, optim_x=optim_x, optim_w=optim_w)\n",
                "def main(params) -> None:\n",
                "    # initialise data\n",
                "    x_dataset, y_dataset = create_data(params, px.core.RKG())\n",
                "\n",
                "    # select testing method\n",
                "    test_on_batch = free_test_on_batch\n",
                "\n",
                "    train_output_per_epoch = []\n",
                "    test_output_per_epoch = []\n",
                "\n",
                "    # these functions are used to initialise the first layer.\n",
                "    node_init_train = get_node_initiator(params[\"init_train\"])\n",
                "    node_init_test = get_node_initiator(params[\"init_test\"])\n",
                "\n",
                "    with tqdm(range(params[\"num_epochs\"]), unit=\"epoch\") as tepoch:\n",
                "        for epoch in tepoch:\n",
                "            tepoch.set_description(f\"Train Epoch {epoch + 1}\")\n",
                "\n",
                "            # train\n",
                "            outputs = []\n",
                "            for (x, y) in zip(x_dataset, y_dataset):\n",
                "                output = train_on_batch(\n",
                "                    loss=loss,\n",
                "                    T=params[\"T\"],\n",
                "                    init_node=node_init_train,\n",
                "                )(\n",
                "                    y, x\n",
                "                )\n",
                "                outputs.append(output)\n",
                "\n",
                "            train_output_per_epoch.append(np.mean(outputs).item())\n",
                "\n",
                "            # test\n",
                "            outputs = []\n",
                "            for (x, y) in zip(x_dataset, y_dataset):\n",
                "                output = test_on_batch(\n",
                "                    loss=loss,\n",
                "                    T=params[\"T\"],\n",
                "                    init_node=node_init_test,\n",
                "                )(\n",
                "                    y, x\n",
                "                )\n",
                "                outputs.append(output)\n",
                "\n",
                "            test_output_per_epoch.append(np.mean(outputs).item())\n",
                "\n",
                "            # todo: add loss, and energy to tqdm bar, add lr_x and lr_w\n",
                "            tepoch.set_postfix(accuracy=test_output_per_epoch[-1])\n",
                "\n",
                "    # plot training loss and test accuracy\n",
                "    plt.clf()\n",
                "    sns.set_theme()\n",
                "    if params[\"dataset\"] == \"xor\":\n",
                "        fig = sns.lineplot(x=range(params[\"num_epochs\"]), y=np.full_like(range(params[\"num_epochs\"]), 0.75, dtype=float), label=\"Linear Classifier Accuracy = 0.75\", color=\"red\")\n",
                "    elif params[\"dataset\"] == \"two_moons\":\n",
                "        fig = sns.lineplot(x=range(params[\"num_epochs\"]), y=np.full_like(range(params[\"num_epochs\"]), 0.866, dtype=float), label=\"Linear Classifier Accuracy = 0.866\", color=\"red\")\n",
                "    fig = sns.lineplot(x=range(params[\"num_epochs\"]), y=test_output_per_epoch, label=\"Test Accuracy\", linewidth = 0.5)\n",
                "    fig.set(ylim=(0, 1.01), title=f\"Test Accuracy. Max={max(test_output_per_epoch):.3f} at epoch {test_output_per_epoch.index(max(test_output_per_epoch))}.\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Train Epoch 65:  50%|█████     | 64/128 [00:58<00:42,  1.51epoch/s, accuracy=0.499]"
                    ]
                }
            ],
            "source": [
                "main(params)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "pcax",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6 (main, Oct  7 2022, 20:19:58) [GCC 11.2.0]"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "f4fd8fe68194e475a108d2c5b5ee1f820870a7a5127298df9ccb3dbbd47c3153"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}