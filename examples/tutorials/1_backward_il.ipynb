{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tutorial 1: **Backward inference learning**\n",
                "In this notebook, you will learn how to use pcax to do backward inference, that is predicting the inputs given the output.\n",
                "Since the library is still in its early development, expect major syntax changes. You can keep coming back to this notebook to stay updated.\n",
                "\n",
                "Good luck!"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 0: Importing dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Core dependencies\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "import optax\n",
                "\n",
                "# pcax\n",
                "import pcax as px\n",
                "import pcax.utils as pxu\n",
                "import pcax.nn as nn\n",
                "\n",
                "# Environment variables\n",
                "import os\n",
                "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
                "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Defining a Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Model(px.EnergyModule):\n",
                "    def __init__(self, input_dim, hidden_dim, output_dim, act_fn) -> None:\n",
                "        super().__init__()\n",
                "\n",
                "        self.act_fn = act_fn\n",
                "\n",
                "        self.fc_layers = [\n",
                "            nn.Linear(input_dim, hidden_dim),\n",
                "            nn.Linear(hidden_dim, hidden_dim),\n",
                "            nn.Linear(hidden_dim, output_dim)\n",
                "        ]\n",
                "\n",
                "        # We need an extra node to store the input\n",
                "        self.pc_nodes = [\n",
                "            px.Node() for _ in range(4)\n",
                "        ]\n",
                "\n",
                "        self.pc_nodes[-1].x.frozen = True\n",
                "\n",
                "    def __call__(self, x, t=None):\n",
                "        # Save the input\n",
                "        x = self.pc_nodes[0](x)[\"x\"]\n",
                "\n",
                "        # Forward pass\n",
                "        x = self.pc_nodes[1](self.act_fn(self.fc_layers[0](x)))[\"x\"]\n",
                "        x = self.pc_nodes[2](self.act_fn(self.fc_layers[1](x)))[\"x\"]\n",
                "\n",
                "        # No activation on the last layer\n",
                "        x = self.pc_nodes[3](self.fc_layers[2](x))[\"x\"]\n",
                "\n",
                "        # Save the target, if given\n",
                "        if t is not None:\n",
                "            self.pc_nodes[3][\"x\"] = t\n",
                "\n",
                "        # Return the output (\"u\" is equal to \"x\" if the target is not fixed,\n",
                "        # while it is the actual output of the model if the target is fixed)\n",
                "        return self.pc_nodes[3][\"u\"]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Defining the data\n",
                "\n",
                "This part is unrelated to pcax. The only highlight is noticing how JAX handles randomness by passing a key around. pcax stores the key internally and allows to access it via a `px.random.RandomKeyGenerator`. We will see later an example of its usage."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.datasets import make_moons\n",
                "\n",
                "def create_data(params, rkey):\n",
                "    rkey1, rkey2 = jax.random.split(rkey)\n",
                "    if params[\"dataset\"] == \"xor\":\n",
                "        x_dataset = jax.random.randint(rkey1, (params[\"num_batches\"], params[\"batch_size\"], 2), 0, 2)\n",
                "        y_dataset = ((jnp.sum(x_dataset, axis=-1) % 2)[:, :, None]  == jnp.arange(2)).astype(jnp.float32)\n",
                "        x_dataset = x_dataset.astype(jnp.float32) + jax.random.normal(rkey2, (params[\"num_batches\"], params[\"batch_size\"], 2)) * params[\"data_noise\"]\n",
                "    elif params[\"dataset\"] == \"two_moons\":\n",
                "        x_dataset, y_dataset = make_moons(n_samples=params[\"batch_size\"], shuffle=True, noise=params[\"data_noise\"], random_state=rkey[0].item())\n",
                "        y_dataset = jax.numpy.stack([1- y_dataset, y_dataset], axis=1)\n",
                "        x_dataset, y_dataset = jnp.array(x_dataset), jnp.array(y_dataset).astype(jnp.float32)\n",
                "        x_dataset, y_dataset = jnp.expand_dims(x_dataset, axis=0), jnp.expand_dims(y_dataset, axis=0)\n",
                "    else:\n",
                "        raise NotImplementedError(f\"Dataset not implemented: {params['dataset']}\")\n",
                "    return x_dataset, y_dataset"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Backward inference\n",
                "Predictive coding allows us to perform backward inference by fixing the output nodes (i.e., the coordinates of a point) and predicting the input nodes (i.e., its label) that minimise the total energy of the network. To achieve so, we need to modify the inference phase. In fact, the training phase (as it always fixes both input and output nodes) does not differ from normal forward inference training."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The main problem to solve in backward inference is how to initialise the value nodes during inference: since we do not have any input value what should we propagate through the network to get the best performance? This is still an open question. In this tutorial we found that the average input node activation (i.e., the vector [0.5, 0.5]) seems to work quite well, however feel free to experiment around it. The network can be configure to use any constant, random noise, etc. as input value."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "params = {\n",
                "    \"dataset\": \"xor\",\n",
                "    \"data_noise\": 0.05,\n",
                "    \"num_epochs\": 128,\n",
                "    \"hidden_dim\": 16,\n",
                "    \"num_batches\": 128,\n",
                "    \"batch_size\": 64,\n",
                "    \"T\": 8,\n",
                "\n",
                "    \"optim_x_l2\": 0.001,\n",
                "    \"optim_x_lr\": 0.5,\n",
                "    \"optim_w_l2\": 0.001,\n",
                "    \"optim_w_lr\": 3e-3,\n",
                "    \"optim_w_momentum\": 0.9,\n",
                "    \"optim_w_nesterov\": True,\n",
                "\n",
                "    # For the training phase we forward the input values\n",
                "    \"init_train\": {\n",
                "        \"init_rand_weight\": 0,\n",
                "        \"init_forward_weight\": 1.0,\n",
                "        \"init_constant\": 0,\n",
                "        \"init_constant_weight\": 0,\n",
                "    },\n",
                "    # For the test phase we initialize the input values with a constant\n",
                "    # since we don't have any input values\n",
                "    \"init_test\": {\n",
                "        \"init_rand_weight\": 0,\n",
                "        \"init_forward_weight\": 0,\n",
                "        \"init_constant\": 0.5,\n",
                "        \"init_constant_weight\": 1.0,\n",
                "    },\n",
                "\n",
                "    \"logs_plot_kwargs\": {},\n",
                "}\n",
                "\n",
                "model = Model(2, params[\"hidden_dim\"], 2, jax.nn.tanh)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To avoid any dependencies between the batch_size and the learning rates we sum over the energies of each input sample and divide the `optim_w_lr` by the batch size."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@pxu.vectorize(px.f(px.NodeParam, with_cache=True), in_axis=(0,), out_axis=(\"sum\",))\n",
                "def loss(x, *, model):\n",
                "    y = model(x)\n",
                "    return model.energy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with pxu.train(model, jax.numpy.zeros((params[\"batch_size\"], 2)), None):\n",
                "    optim_x = pxu.Optim(\n",
                "        optax.chain(\n",
                "            optax.add_decayed_weights(weight_decay=params[\"optim_x_l2\"]),\n",
                "            optax.sgd(params[\"optim_x_lr\"])\n",
                "        ),\n",
                "        model.parameters().filter(px.f(px.NodeParam)(frozen=False)),\n",
                "        allow_none_grads=True,\n",
                "    )\n",
                "    optim_w = pxu.Optim(\n",
                "        optax.chain(\n",
                "            optax.add_decayed_weights(weight_decay=params[\"optim_w_l2\"]),\n",
                "            optax.sgd(params[\"optim_w_lr\"] / params[\"batch_size\"], momentum=params[\"optim_w_momentum\"], nesterov=params[\"optim_w_nesterov\"])\n",
                "        ),\n",
                "        model.parameters().filter(px.f(px.LayerParam)),\n",
                "    )"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`get_node_initiator` generates the value to pass to `px.init_nodes`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_node_initiator(params):\n",
                "    def node_initiator(input = None, shape = None, key = None):\n",
                "        if shape is None:\n",
                "            shape = input.shape\n",
                "        if input is None:\n",
                "            input = jnp.zeros(shape)\n",
                "        r = jax.random.normal(key, shape) * params[\"init_rand_weight\"]\n",
                "        d = input * params[\"init_forward_weight\"]\n",
                "        c = jnp.full(shape, params[\"init_constant\"]) * params[\"init_constant_weight\"]\n",
                "        return r + d + c\n",
                "    return node_initiator"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Differently from forward inference, we need to perform an iterative phase in order to retrieve the correct value from the input layer (our output). Notice how `y = model.pc0[\"x\"]`, which is the first layer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pcax.core as pxc\n",
                "\n",
                "@pxu.jit()\n",
                "def free_test_on_batch(t, x, *, model, optim_x, loss, T, init_node):\n",
                "    forward_value = init_node(t, t.shape, pxc.RKG())\n",
                "    with pxu.eval(model, forward_value, x):\n",
                "        for i in range(T):\n",
                "            with pxu.step(model):\n",
                "                # Notice how we pass in input model.pc0[\"x\"], in this way\n",
                "                # the energy of the first layer is 0 and the value nodes are\n",
                "                # influenced only by the following layers.\n",
                "                g, (l,) = pxu.grad_and_values(\n",
                "                    px.f(px.NodeParam)(frozen=False),\n",
                "                )(loss)(\n",
                "                    model.pc_nodes[0][\"x\"],\n",
                "                    model = model\n",
                "                )\n",
                "                \n",
                "                optim_x(g)\n",
                "        \n",
                "        y = model.pc_nodes[0][\"x\"]\n",
                "    \n",
                "    target_class = jnp.argmax(t, axis=1)\n",
                "    predicted_class = jnp.argmax(y, axis=1)\n",
                "    accuracy = jnp.mean(predicted_class == target_class)\n",
                "\n",
                "    return accuracy"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We use PPC to train the model, that is we update nodes and weights at the same time. This seems to improve performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@pxu.jit()\n",
                "def train_on_batch(t, x, *, model, optim_x, optim_w, loss, T, init_node):\n",
                "    model.pc_nodes[0].x.frozen = True\n",
                "\n",
                "    forward_value = init_node(t, t.shape, pxc.RKG())\n",
                "    with pxu.train(model, forward_value, x):\n",
                "        for i in range(T):\n",
                "            with pxu.step(model):\n",
                "                g, (l,) = pxu.grad_and_values(\n",
                "                    px.f(px.NodeParam)(frozen=False) | px.f(px.LayerParam),\n",
                "                )(loss)(forward_value, model=model)\n",
                "\n",
                "                optim_x(g)\n",
                "                optim_w(g)\n",
                "\n",
                "        y = model.pc_nodes[0][\"x\"]\n",
                "\n",
                "    target_class = jnp.argmax(t, axis=1)\n",
                "    predicted_class = jnp.argmax(y, axis=1)\n",
                "    accuracy = jnp.mean(predicted_class == target_class)\n",
                "\n",
                "    model.pc_nodes[0].x.frozen = False\n",
                "\n",
                "    return accuracy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "\n",
                "x_dataset, y_dataset = create_data(params, pxc.RKG())\n",
                "def main(params, *, model, optim_x, optim_w) -> None:\n",
                "    # initialise data\n",
                "\n",
                "    # select testing method\n",
                "    test_on_batch = free_test_on_batch\n",
                "\n",
                "    train_output_per_epoch = []\n",
                "    test_output_per_epoch = []\n",
                "\n",
                "    # these functions are used to initialise the first layer.\n",
                "    node_init_train = get_node_initiator(params[\"init_train\"])\n",
                "    node_init_test = get_node_initiator(params[\"init_test\"])\n",
                "\n",
                "    train_fn = train_on_batch.snapshot(\n",
                "        model=model,\n",
                "        optim_x=optim_x,\n",
                "        optim_w=optim_w,\n",
                "        loss=loss,\n",
                "        T=params[\"T\"],\n",
                "        init_node=node_init_train\n",
                "    )\n",
                "    test_fn = test_on_batch.snapshot(\n",
                "        model=model,\n",
                "                    optim_x=optim_x,\n",
                "                    loss=loss,\n",
                "                    T=params[\"T\"],\n",
                "                    init_node=node_init_test,\n",
                "    )\n",
                "\n",
                "    with tqdm(range(params[\"num_epochs\"]), unit=\"epoch\") as tepoch:\n",
                "        for epoch in tepoch:\n",
                "            tepoch.set_description(f\"Train Epoch {epoch + 1}\")\n",
                "\n",
                "            # train\n",
                "            outputs = []\n",
                "            for (x, y) in zip(x_dataset, y_dataset):\n",
                "                output = train_fn(\n",
                "                    y, x\n",
                "                )\n",
                "                outputs.append(output)\n",
                "\n",
                "            train_output_per_epoch.append(np.mean(outputs).item())\n",
                "\n",
                "            # test\n",
                "            outputs = []\n",
                "            for (x, y) in zip(x_dataset, y_dataset):\n",
                "                output = test_fn(\n",
                "                    y, x,\n",
                "                )\n",
                "                outputs.append(output)\n",
                "\n",
                "            test_output_per_epoch.append(np.mean(outputs).item())\n",
                "\n",
                "            # todo: add loss, and energy to tqdm bar, add lr_x and lr_w\n",
                "            tepoch.set_postfix(accuracy=test_output_per_epoch[-1])\n",
                "\n",
                "            if test_output_per_epoch[-1] > 0.95:\n",
                "                break\n",
                "\n",
                "    # plot training loss and test accuracy\n",
                "    plt.clf()\n",
                "    sns.set_theme()\n",
                "    if params[\"dataset\"] == \"xor\":\n",
                "        fig = sns.lineplot(x=range(params[\"num_epochs\"]), y=np.full_like(range(params[\"num_epochs\"]), 0.75, dtype=float), label=\"Linear Classifier Accuracy = 0.75\", color=\"red\")\n",
                "    elif params[\"dataset\"] == \"two_moons\":\n",
                "        fig = sns.lineplot(x=range(params[\"num_epochs\"]), y=np.full_like(range(params[\"num_epochs\"]), 0.866, dtype=float), label=\"Linear Classifier Accuracy = 0.866\", color=\"red\")\n",
                "    fig = sns.lineplot(x=range(params[\"num_epochs\"]), y=test_output_per_epoch, label=\"Test Accuracy\", linewidth = 0.5)\n",
                "    fig.set(ylim=(0, 1.01), title=f\"Test Accuracy. Max={max(test_output_per_epoch):.3f} at epoch {test_output_per_epoch.index(max(test_output_per_epoch))}.\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "main(params, model=model, optim_x=optim_x, optim_w=optim_w)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "pcax",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.2"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "f4fd8fe68194e475a108d2c5b5ee1f820870a7a5127298df9ccb3dbbd47c3153"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
