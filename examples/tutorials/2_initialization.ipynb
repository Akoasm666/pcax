{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: **Node initialization**\n",
    "In this notebook, you will learn how to use pcax to perform custom node initialization for you complex models.\n",
    "Since the library is still in its early development, expect major syntax changes. You can keep coming back to this notebook to stay updated.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "# pcax\n",
    "import pcax as px\n",
    "import pcax.core as pxc\n",
    "import pcax.utils as pxu\n",
    "import pcax.nn as nn\n",
    "\n",
    "# Environment variables\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining a Model\n",
    "In addition to previous tutorials, we define a model that accepts a custom initialization function that can be used to initialize `px.Node()`.\n",
    "\n",
    "In tutorial #0, we saw that \"the syntax `node(x)[\"x\"]` is, by default, a shortcut for the following:\n",
    "```python\n",
    "node[\"u\"] = x\n",
    "if node.is_init:\n",
    "    node[\"x\"] = node[\"u\"]\n",
    "return node[\"x\"]\n",
    "```\n",
    "while it is actually:\n",
    "```python\n",
    "node[\"u\"] = x\n",
    "if node.is_init:\n",
    "    # rkg is a random key generator that is always passed to __call__\n",
    "    init_fn(node, rkg)\n",
    "return node[\"x\"]\n",
    "```\n",
    "so we can customize `init_fn` behaviour.\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the default function used to initialize the nodes.\n",
    "def forward_init(node: px.Node, rkg: pxc.RandomKeyGenerator):\n",
    "    node[\"x\"] = node[\"u\"]\n",
    "\n",
    "# This initializes the nodes with zeros. Consider that, given the presence of the bias term,\n",
    "# this may be equivalent to initializing the nodes with a constant value (or even the node average:\n",
    "# you force the average to be zero). The last comment doesn't apply to nodes observed during training,\n",
    "# as their average may not be zero.\n",
    "def zero_init(node: px.Node, rkg: pxc.RandomKeyGenerator):\n",
    "    node[\"x\"] = jnp.zeros(node[\"u\"].shape)\n",
    "\n",
    "# This initializes the nodes with a random normal distribution, which is the original initialization\n",
    "# method used in predictive coding (of course a std of 1 may be too high, but this is just an example)\n",
    "def random_init(node: px.Node, rkg: pxc.RandomKeyGenerator, std: float = 1.):\n",
    "    node[\"x\"] = jax.random.normal(rkg(), node[\"u\"].shape) * std\n",
    "\n",
    "# It may also be the case that we don't want to initialize the nodes at all. This is useful when\n",
    "# we want to reuse the node values in the next training batch. This is an area yet to be explored.\n",
    "# We will try to make the first steps in this notebook.\n",
    "def no_init(node: px.Node, rkg: pxc.RandomKeyGenerator):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to use a different energy function from (M)SE (we don't actually do the mean as we want the error to be individually propagated to each batched sample in the value nodes) as in classification tasks CE is normally the go-to choice. Thus, we create a different energy function to be used by the last `px.Node`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_energy(node: px.Node, rkg: pxc.RandomKeyGenerator):\n",
    "    u = jax.nn.softmax(node[\"u\"])\n",
    "    return (node[\"x\"] * jnp.log(node[\"x\"] / (u + 1e-8) + 1e-8)).sum(axis=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can define the model as before, but with an extra argument: `init_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(px.EnergyModule):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 output_dim: int,\n",
    "                 nm_layers: int,\n",
    "                 act_fn: Callable[[jax.Array], jax.Array],\n",
    "                 init_fn: Optional[Callable[[px.Node, pxc.RandomKeyGenerator], None]] = None\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.act_fn = act_fn\n",
    "        self.layers = [nn.Linear(input_dim, hidden_dim)] + [\n",
    "            nn.Linear(hidden_dim, hidden_dim) for _ in range(nm_layers - 1)\n",
    "        ] + [nn.Linear(hidden_dim, output_dim)]\n",
    "\n",
    "        self.nodes = [px.Node(init_fn=init_fn) for _ in range(nm_layers)] + [px.Node(energy_fn=ce_energy)]\n",
    "\n",
    "        self.nodes[-1].x.frozen = True\n",
    "\n",
    "    def __call__(self,\n",
    "                 x: jax.Array,\n",
    "                 t: Optional[jax.Array] = None):\n",
    "        \n",
    "        for node, layer in zip(self.nodes[:-1], self.layers[:-1]):\n",
    "            x = node(self.act_fn(layer(x)))[\"x\"]\n",
    "\n",
    "        x = self.nodes[-1](self.layers[-1](x))[\"x\"]\n",
    "\n",
    "        if t is not None:\n",
    "            self.nodes[-1][\"x\"] = t\n",
    "        \n",
    "        return self.nodes[-1][\"u\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define all the functions necessary to train a model. Note how compared to tutorial #0, we change the definitions order. This is because, normally, the model will be the last entity defined and these training functions may actually be defined in imported files in order to be reused."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data-loading utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to convert the square 0-255 images to 0-1 float vectors.\n",
    "class FlattenAndCast:\n",
    "    def __call__(self, pic):\n",
    "        return np.ravel(np.array(pic, dtype=np.float32) / 255.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loos function (nothing new):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxu.vectorize(px.f(px.NodeParam, with_cache=True), in_axis=(0, 0), out_axis=(\"sum\",))\n",
    "def loss(x: jax.Array, t: Optional[jax.Array] = None, *, model):\n",
    "    model(x)\n",
    "    return model.energy()\n",
    "\n",
    "train_x = pxu.grad_and_values(\n",
    "    px.f(px.NodeParam)(frozen=False),\n",
    ")(loss)\n",
    "\n",
    "train_w = pxu.grad_and_values(\n",
    "    px.f(px.LayerParam)\n",
    ")(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxu.jit()\n",
    "def train_batch(x, y, *, T, model, optim_w, optim_x):\n",
    "    y = jax.nn.one_hot(y, 10)\n",
    "    with pxu.train(model, x, y) as (y_hat,):\n",
    "        for i in range(T):\n",
    "            with pxu.step(model):\n",
    "                g, (v,) = train_x(x, y, model=model)\n",
    "                optim_x(g)\n",
    "\n",
    "        with pxu.step(model):\n",
    "            g, (v,) = train_w(x, y, model=model)\n",
    "            optim_w(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxu.jit()\n",
    "def eval_batch(x, y, *, model, optim_x):\n",
    "    y = jax.nn.one_hot(y, 10)\n",
    "    with pxu.eval(model, x, y) as (y_hat,):\n",
    "        return (y_hat.argmax(-1) == y.argmax(-1)).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch and testing functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(dl, train_fn, p):\n",
    "    for i, batch in enumerate(dl):\n",
    "        x, y = batch\n",
    "\n",
    "        train_fn(x, y)\n",
    "        \n",
    "        if i / len(dl) >= p:\n",
    "            break\n",
    "\n",
    "def test(dl, eval_fn):\n",
    "    accuracies = []\n",
    "    for batch in dl:\n",
    "        x, y = batch\n",
    "\n",
    "        accuracies.append(eval_fn(x, y))\n",
    "\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, the `__main__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:08<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.88%\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define parameters\n",
    "params = {\n",
    "        \"batch_size\": 256,\n",
    "        \"x_learning_rate\": 0.05,\n",
    "        \"w_learning_rate\": 1e-4,\n",
    "        \"num_epochs\": 8,\n",
    "        \"num_layers\": 4,\n",
    "        \"hidden_dim\": 128,\n",
    "        \"input_dim\": 28 * 28,\n",
    "        \"output_dim\": 10,\n",
    "        \"T\": 4,\n",
    "        \"p\": 1.0\n",
    "    }\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataset = FashionMNIST(\n",
    "    \"/tmp/mnist/\",\n",
    "    transform=FlattenAndCast(),\n",
    "    download=True,\n",
    "    train=True,\n",
    ")\n",
    "train_dataloader = pxu.data.TorchDataloader(\n",
    "    train_dataset,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataset = FashionMNIST(\n",
    "    \"/tmp/mnist/\",\n",
    "    transform=FlattenAndCast(),\n",
    "    download=True,\n",
    "    train=False,\n",
    ")\n",
    "test_dataloader = pxu.data.TorchDataloader(\n",
    "    test_dataset,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create model\n",
    "    model = Model(28 * 28, params[\"hidden_dim\"], 10, params[\"num_layers\"], jax.nn.gelu, forward_init)\n",
    "\n",
    "    # Create optimizers\n",
    "    with pxu.train(model, jnp.zeros((params[\"batch_size\"], 28 * 28)), None):\n",
    "        optim_x = pxu.Optim(\n",
    "            optax.sgd(params[\"x_learning_rate\"]),\n",
    "            model.parameters().filter(px.f(px.NodeParam)(frozen=False))\n",
    "        )\n",
    "        optim_w = pxu.Optim(\n",
    "            optax.adam(params[\"w_learning_rate\"]),\n",
    "            model.parameters().filter(px.f(px.LayerParam)),\n",
    "        )\n",
    "\n",
    "    # Create snapshots\n",
    "    train_fn = train_batch.snapshot(T=params[\"T\"], model=model, optim_x=optim_x, optim_w=optim_w)\n",
    "    evaluate_fn = eval_batch.snapshot(model=model, optim_x=optim_w)\n",
    "\n",
    "    # Train:\n",
    "    for e in tqdm(range(params[\"num_epochs\"])):\n",
    "        epoch(train_dataloader, train_fn, params[\"p\"])\n",
    "\n",
    "    # Evaluation:\n",
    "    accuracy = test(test_dataloader, evaluate_fn)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Different initialization techniques\n",
    "Let's now try with a different initialization: `zero_init`. Note however, that we still want to perform a forward init during evaluation as it guarantees to obtain the lowest energy. Thus, we want a custom initialization function that behaves differently during training and evaluation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(node: px.Node, rkg: pxc.RandomKeyGenerator):\n",
    "    if node.is_train:\n",
    "        node[\"x\"] = jnp.zeros_like(node[\"u\"])\n",
    "    else:\n",
    "        node[\"x\"] = node[\"u\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:14<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.88%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = Model(28 * 28, params[\"hidden_dim\"], 10, params[\"num_layers\"], jax.nn.tanh, zero_init)\n",
    "    \n",
    "    # Create optimizers\n",
    "    with pxu.train(model, jnp.zeros((params[\"batch_size\"], 28 * 28)), None):\n",
    "        optim_x = pxu.Optim(\n",
    "            optax.sgd(params[\"x_learning_rate\"]),\n",
    "            model.parameters().filter(px.f(px.NodeParam)(frozen=False))\n",
    "        )\n",
    "        optim_w = pxu.Optim(\n",
    "            optax.adam(params[\"w_learning_rate\"]),\n",
    "            model.parameters().filter(px.f(px.LayerParam)),\n",
    "        )\n",
    "\n",
    "    # Create snapshots\n",
    "    train_fn = train_batch.snapshot(T=params[\"T\"], model=model, optim_x=optim_x, optim_w=optim_w)\n",
    "    eval_fn = eval_batch.snapshot(model=model, optim_x=optim_w)\n",
    "\n",
    "    # Train:\n",
    "    for e in tqdm(range(params[\"num_epochs\"])):\n",
    "        epoch(train_dataloader, train_fn, params[\"p\"])\n",
    "\n",
    "    # Evaluation:\n",
    "    accuracy = test(test_dataloader, eval_fn)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `zero_init` does not work! This is because the model performs only `T=4` x steps with a low learning rate of `0.05`. These settings only work when the necessary x correction is small enough (target vs current x values). We can try to solve this problem by performing more x steps and using a larger learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:23<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.09%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = Model(28 * 28, params[\"hidden_dim\"], 10, params[\"num_layers\"], jax.nn.tanh, zero_init)\n",
    "    \n",
    "    # Create optimizers\n",
    "    with pxu.train(model, jnp.zeros((params[\"batch_size\"], 28 * 28)), None):\n",
    "        optim_x = pxu.Optim(\n",
    "            optax.sgd(0.5),\n",
    "            model.parameters().filter(px.f(px.NodeParam)(frozen=False))\n",
    "        )\n",
    "        optim_w = pxu.Optim(\n",
    "            optax.adam(params[\"w_learning_rate\"]),\n",
    "            model.parameters().filter(px.f(px.LayerParam)),\n",
    "        )\n",
    "\n",
    "    # Create snapshots\n",
    "    train_fn = train_batch.snapshot(T=32, model=model, optim_x=optim_x, optim_w=optim_w)\n",
    "    eval_fn = eval_batch.snapshot(model=model, optim_x=optim_w)\n",
    "\n",
    "    # Train:\n",
    "    for e in tqdm(range(params[\"num_epochs\"])):\n",
    "        epoch(train_dataloader, train_fn, params[\"p\"])\n",
    "\n",
    "    # Evaluation:\n",
    "    accuracy = test(test_dataloader, eval_fn)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Custom initialization\n",
    "Until now, we have been using `pxu.train` and `pxu.eval` to perform automatic initialization of the node values. However, this can also be performed manually for more flexibility. Here, we will initialize the nodes with the class average of the values computed for the previous batch. Note that the evaluation will still be based on forward initialization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxu.jit()\n",
    "def train_batch_avg_init(x, y, nodes_avg_by_class, *, T, model, optim_w, optim_x):\n",
    "    with pxu.train(model, x):\n",
    "        # Initialize nodes\n",
    "        if nodes_avg_by_class is not None:\n",
    "            for node, avg_by_class in zip(model.nodes[1:-1], nodes_avg_by_class):\n",
    "                node[\"x\"] = jax.vmap(lambda y, avg: jax.lax.select_n(y, *jax.numpy.vsplit(avg, 10)).squeeze(), in_axes=(0, None), out_axes=0)(y, avg_by_class)\n",
    "\n",
    "        # Convert y to one_hot\n",
    "        y = jax.nn.one_hot(y, 10)\n",
    "\n",
    "        # Set last node to y\n",
    "        model.nodes[-1][\"x\"] = y\n",
    "\n",
    "        for i in range(T):\n",
    "            with pxu.step(model):\n",
    "                g, (v,) = train_x(x, y, model=model)\n",
    "                optim_x(g)\n",
    "\n",
    "        with pxu.step(model):\n",
    "            g, (v,) = train_w(x, y, model=model)\n",
    "            optim_w(g)\n",
    "\n",
    "    nodes_avg_by_class = [\n",
    "        jnp.sum(jnp.reshape(jnp.tile(node[\"x\"], (10, 1)), (10, x.shape[0], -1)) * jnp.expand_dims(y.T, axis=-1), axis=1)\n",
    "        / jnp.expand_dims(jnp.sum(y, axis=0), axis=-1) for node in model.nodes[1:-1]\n",
    "    ]\n",
    "\n",
    "    return nodes_avg_by_class\n",
    "\n",
    "\n",
    "def epoch_avg_init(dl, train_fn, p):\n",
    "    nodes_avg_by_class = None\n",
    "\n",
    "    for i, batch in enumerate(dl):\n",
    "        x, y = batch\n",
    "        nodes_avg_by_class = train_fn(x, y, nodes_avg_by_class)\n",
    "\n",
    "        if i / len(dl) > p:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.08%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = Model(28 * 28, params[\"hidden_dim\"], 10, params[\"num_layers\"], jax.nn.gelu)\n",
    "    \n",
    "    # Create optimizers\n",
    "    with pxu.train(model, jax.random.normal(pxc.RKG(), (params[\"batch_size\"], 28 * 28)), None):\n",
    "        optim_x = pxu.Optim(\n",
    "            optax.sgd(params[\"x_learning_rate\"]),\n",
    "            model.parameters().filter(px.f(px.NodeParam)(frozen=False))\n",
    "        )\n",
    "        optim_w = pxu.Optim(\n",
    "            optax.adam(params[\"w_learning_rate\"]),\n",
    "            model.parameters().filter(px.f(px.LayerParam)),\n",
    "        )\n",
    "    \n",
    "\n",
    "    # Create snapshots\n",
    "    train_fn_avg_init = train_batch_avg_init.snapshot(T=params[\"T\"], model=model, optim_x=optim_x, optim_w=optim_w)\n",
    "    eval_fn = eval_batch.snapshot(model=model, optim_x=optim_w)\n",
    "\n",
    "    # Train:\n",
    "    for e in tqdm(range(params[\"num_epochs\"])):\n",
    "        epoch_avg_init(train_dataloader, train_fn_avg_init, params[\"p\"])\n",
    "\n",
    "    # Evaluation:\n",
    "    accuracy = test(test_dataloader, eval_fn)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
