{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: **Node initialization**\n",
    "In this notebook, you will learn how to use pcax to perform custom node initialization for you complex models.\n",
    "Since the library is still in its early development, expect major syntax changes. You can keep coming back to this notebook to stay updated.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "# Core dependencies\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "# pcax\n",
    "import pcax as px\n",
    "import pcax.core as pxc\n",
    "import pcax.utils as pxu\n",
    "import pcax.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining a Model\n",
    "In addition to previous tutorials, we define a model that accepts a custom initialization function that can be used to initialize `px.Node()`.\n",
    "\n",
    "In tutorial #0, we saw that \"the syntax `node(x)[\"x\"]` is, by default, a shortcut for the following:\n",
    "```python\n",
    "node[\"u\"] = x\n",
    "if node.is_init:\n",
    "    node[\"x\"] = node[\"u\"]\n",
    "return node[\"x\"]\n",
    "```\n",
    "while it is actually:\n",
    "```python\n",
    "node[\"u\"] = x\n",
    "if node.is_init:\n",
    "    # rkg is a random key generator that is always passed to __call__\n",
    "    init_fn(node, rkg)\n",
    "return node[\"x\"]\n",
    "```\n",
    "so we can customize `init_fn` behaviour.\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the default function used to initialize the nodes.\n",
    "def forward_init(node: px.Node, rkg: pxc.RandomKeyGenerator):\n",
    "    node[\"x\"] = node[\"u\"]\n",
    "\n",
    "# This initializes the nodes with zeros. Consider that, given the presence of the bias term,\n",
    "# this may be equivalent to initializing the nodes with a constant value (or even the node average:\n",
    "# you force the average to be zero). The last comment doesn't apply to nodes observed during training,\n",
    "# as their average may not be zero.\n",
    "def zero_init(node: px.Node, rkg: pxc.RandomKeyGenerator):\n",
    "    node[\"x\"] = jnp.zeros(node[\"u\"].shape)\n",
    "\n",
    "# This initializes the nodes with a random normal distribution, which is the original initialization\n",
    "# method used in predictive coding (of course a std of 1 may be too high, but this is just an example)\n",
    "def random_init(node: px.Node, rkg: pxc.RandomKeyGenerator, std: float = 1.):\n",
    "    node[\"x\"] = jax.random.normal(rkg(), node[\"u\"].shape) * std\n",
    "\n",
    "# It may also be the case that we don't want to initialize the nodes at all. This is useful when\n",
    "# we want to reuse the node values in the next training batch. This is an area yet to be explored.\n",
    "# We will try to make the first steps in this notebook.\n",
    "def no_init(node: px.Node, rkg: pxc.RandomKeyGenerator):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to use a different energy function from (M)SE (we don't actually do the mean as we want the error to be individually propagated to each batched sample in the value nodes) as in classification tasks CE is normally the go-to choice. Thus, we create a different energy function to be used by the last `px.Node`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_energy(node: px.Node, rkg: pxc.RandomKeyGenerator):\n",
    "    u = jax.nn.softmax(node[\"u\"])\n",
    "    return (node[\"x\"] * jnp.log(node[\"x\"] / (u + 1e-8) + 1e-8)).sum(axis=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can define the model as before, but with an extra argument: `init_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(px.EnergyModule):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 output_dim: int,\n",
    "                 nm_layers: int,\n",
    "                 act_fn: Callable[[jax.Array], jax.Array],\n",
    "                 init_fn: Optional[Callable[[px.Node, pxc.RandomKeyGenerator], None]] = None\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # We save the number of classes for later use\n",
    "        self.nm_classes = output_dim\n",
    "        self.act_fn = act_fn\n",
    "        self.layers = [nn.Linear(input_dim, hidden_dim)] + [\n",
    "            nn.Linear(hidden_dim, hidden_dim) for _ in range(nm_layers - 1)\n",
    "        ] + [nn.Linear(hidden_dim, output_dim)]\n",
    "\n",
    "        self.nodes = [px.Node(init_fn=init_fn) for _ in range(nm_layers)] + [px.Node(energy_fn=ce_energy)]\n",
    "\n",
    "        self.nodes[-1].x.frozen = True\n",
    "\n",
    "    # We add the new parameter `forward_to` that allows us to forward only up to a certain node.\n",
    "    # This is useful when we want to reuse the node values in the next training batch which is\n",
    "    # what we will do in part 3 of this tutorial.\n",
    "    def __call__(self,\n",
    "                 x: jax.Array,\n",
    "                 t: Optional[jax.Array] = None,\n",
    "                 forward_to: Optional[int] = None):\n",
    "        if forward_to is None:\n",
    "            forward_to = len(self.nodes)\n",
    "        \n",
    "        for i, (node, layer) in enumerate(zip(self.nodes[:forward_to], self.layers[:forward_to])):\n",
    "            act_fn = self.act_fn if i != len(self.nodes) - 1 else lambda x: x\n",
    "            x = node(act_fn(layer(x)))[\"x\"]\n",
    "\n",
    "        if t is not None:\n",
    "            self.nodes[-1][\"x\"] = t\n",
    "        \n",
    "        return self.nodes[-1][\"u\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define all the functions necessary to train a model. Note how, compared to tutorial #0, we change the definition order. This is because, normally, the model will be the last entity defined and these training functions may actually be defined in imported files in order to be reused."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data-loading utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to convert the square 0-255 images to 0-1 float vectors.\n",
    "class FlattenAndCast:\n",
    "    def __call__(self, pic):\n",
    "        return np.ravel(np.array(pic, dtype=np.float32) / 255.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function (nothing new):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxu.vectorize(px.f(px.NodeParam, with_cache=True), in_axis=(0, 0), out_axis=(\"sum\",))\n",
    "def loss(x: jax.Array, *, model):\n",
    "    model(x)\n",
    "    return model.energy()\n",
    "\n",
    "train_x = pxu.grad_and_values(\n",
    "    px.f(px.NodeParam)(frozen=False),\n",
    ")(loss)\n",
    "\n",
    "train_w = pxu.grad_and_values(\n",
    "    px.f(px.LayerParam)\n",
    ")(loss)\n",
    "\n",
    "train = pxu.grad_and_values(\n",
    "    px.f(px.LayerParam) | px.f(px.NodeParam)(frozen=False)\n",
    ")(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxu.jit()\n",
    "def train_batch(x, y, *, T, model, optim_w, optim_x):\n",
    "    y = jax.nn.one_hot(y, 10)\n",
    "\n",
    "    with pxu.train(model, x, y):\n",
    "        for i in range(T):\n",
    "            with pxu.step(model):\n",
    "                g, (v,) = train_x(x, y, model=model)\n",
    "                optim_x(g)\n",
    "\n",
    "        with pxu.step(model):\n",
    "            g, (v,) = train_w(x, y, model=model)\n",
    "            optim_w(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxu.jit()\n",
    "def eval_batch(x, y, *, model):\n",
    "    y = jax.nn.one_hot(y, 10)\n",
    "    with pxu.eval(model, x, y) as (y_hat,):\n",
    "        return (y_hat.argmax(-1) == y.argmax(-1)).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch and testing functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(dl, train_fn):\n",
    "    for batch in dl:\n",
    "        x, y = batch\n",
    "\n",
    "        train_fn(x, y)\n",
    "\n",
    "def test(dl, eval_fn):\n",
    "    accuracies = []\n",
    "    for batch in dl:\n",
    "        x, y = batch\n",
    "\n",
    "        accuracies.append(eval_fn(x, y))\n",
    "\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, the `__main__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:16<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.11%\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define parameters\n",
    "params = {\n",
    "        \"batch_size\": 300,\n",
    "        \"x_learning_rate\": 0.05,\n",
    "        \"w_learning_rate\": 5e-4,\n",
    "        \"num_epochs\": 16,\n",
    "        \"num_layers\": 4,\n",
    "        \"hidden_dim\": 256,\n",
    "        \"input_dim\": 28 * 28,\n",
    "        \"output_dim\": 10,\n",
    "        \"T\": 8\n",
    "    }\n",
    "\n",
    "# Create dataloaders\n",
    "# We use the custom sampler `pxu.data.BatchAlignedSampler` to ensure that each batch contains\n",
    "# the same number of samples from each class and in the same order. This is important if we want\n",
    "# to reuse the node values in the next training batch. This introduces the constraint of having a\n",
    "# batch size which is a multiple of the number of classes and only works well with balanced datasets.\n",
    "# With some fancy masking and indexing this can is not necessary, but in this way it is easier to code\n",
    "# and understand (and slighlty faster to execute). By default, the `BatchAlignedSampler` shuffles\n",
    "# the indices, but we disable this behavior for the test dataloader.\n",
    "train_dataset = FashionMNIST(\n",
    "    \"/tmp/mnist/\",\n",
    "    transform=FlattenAndCast(),\n",
    "    download=True,\n",
    "    train=True,\n",
    ")\n",
    "train_dataloader = pxu.data.TorchDataloader(\n",
    "    train_dataset,\n",
    "    batch_sampler=pxu.data.BatchAlignedSampler(train_dataset, params[\"batch_size\"]),\n",
    "    num_workers=8,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataset = FashionMNIST(\n",
    "    \"/tmp/mnist/\",\n",
    "    transform=FlattenAndCast(),\n",
    "    download=True,\n",
    "    train=False,\n",
    ")\n",
    "test_dataloader = pxu.data.TorchDataloader(\n",
    "    test_dataset,\n",
    "    batch_sampler=pxu.data.BatchAlignedSampler(test_dataset, params[\"batch_size\"], shuffle=False),\n",
    "    num_workers=8,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create model. `forward_init` is the default behavior, we specify it here for clarity.\n",
    "    model = Model(28 * 28, params[\"hidden_dim\"], 10, params[\"num_layers\"], jax.nn.gelu, forward_init)\n",
    "\n",
    "    # Create optimizers\n",
    "    with pxu.train(model, jnp.zeros((params[\"batch_size\"], 28 * 28)), None):\n",
    "        optim_x = pxu.Optim(\n",
    "            optax.sgd(params[\"x_learning_rate\"]),\n",
    "            model.parameters().filter(px.f(px.NodeParam)(frozen=False))\n",
    "        )\n",
    "        optim_w = pxu.Optim(\n",
    "            optax.adamw(params[\"w_learning_rate\"]),\n",
    "            model.parameters().filter(px.f(px.LayerParam)),\n",
    "        )\n",
    "\n",
    "    # Create snapshots\n",
    "    train_fn = train_batch.snapshot(T=params[\"T\"], model=model, optim_x=optim_x, optim_w=optim_w)\n",
    "    evaluate_fn = eval_batch.snapshot(model=model)\n",
    "\n",
    "    # Train:\n",
    "    for e in tqdm(range(params[\"num_epochs\"])):\n",
    "        epoch(train_dataloader, train_fn)\n",
    "\n",
    "    # Evaluation:\n",
    "    accuracy = test(test_dataloader, evaluate_fn)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Different initialization techniques\n",
    "Let's now try with a different initialization: `zero_init`. Note however, that we still want to perform a forward init during evaluation as it guarantees to obtain the lowest energy. Thus, we want a custom initialization function that behaves differently during training and evaluation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(node: px.Node, rkg: pxc.RandomKeyGenerator):\n",
    "    if node.is_train:\n",
    "        node[\"x\"] = jnp.zeros_like(node[\"u\"])\n",
    "    else:\n",
    "        node[\"x\"] = node[\"u\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:15<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 11.25%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = Model(28 * 28, params[\"hidden_dim\"], 10, params[\"num_layers\"], jax.nn.gelu, zero_init)\n",
    "    \n",
    "    # Create optimizers\n",
    "    with pxu.train(model, jnp.zeros((params[\"batch_size\"], 28 * 28)), None):\n",
    "        optim_x = pxu.Optim(\n",
    "            optax.sgd(params[\"x_learning_rate\"]),\n",
    "            model.parameters().filter(px.f(px.NodeParam)(frozen=False))\n",
    "        )\n",
    "        optim_w = pxu.Optim(\n",
    "            optax.adamw(params[\"w_learning_rate\"]),\n",
    "            model.parameters().filter(px.f(px.LayerParam)),\n",
    "        )\n",
    "\n",
    "    # Create snapshots\n",
    "    train_fn = train_batch.snapshot(T=params[\"T\"], model=model, optim_x=optim_x, optim_w=optim_w)\n",
    "    eval_fn = eval_batch.snapshot(model=model)\n",
    "\n",
    "    # Train:\n",
    "    for e in tqdm(range(params[\"num_epochs\"])):\n",
    "        epoch(train_dataloader, train_fn)\n",
    "\n",
    "    # Evaluation:\n",
    "    accuracy = test(test_dataloader, eval_fn)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `zero_init` does not work! This is because the model performs only `T=8` x steps with a low learning rate of `0.05`. These settings only work when the necessary x correction is small enough (target vs current x values). We can try to solve this problem by performing more x steps and using a larger learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:24<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.75%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = Model(28 * 28, params[\"hidden_dim\"], 10, params[\"num_layers\"], jax.nn.gelu, zero_init)\n",
    "    \n",
    "    # Create optimizers\n",
    "    with pxu.train(model, jnp.zeros((params[\"batch_size\"], 28 * 28)), None):\n",
    "        optim_x = pxu.Optim(\n",
    "            optax.sgd(0.1),\n",
    "            model.parameters().filter(px.f(px.NodeParam)(frozen=False))\n",
    "        )\n",
    "        optim_w = pxu.Optim(\n",
    "            optax.adamw(params[\"w_learning_rate\"]),\n",
    "            model.parameters().filter(px.f(px.LayerParam)),\n",
    "        )\n",
    "\n",
    "    # Create snapshots\n",
    "    train_fn = train_batch.snapshot(T=32, model=model, optim_x=optim_x, optim_w=optim_w)\n",
    "    eval_fn = eval_batch.snapshot(model=model)\n",
    "\n",
    "    # Train:\n",
    "    for e in tqdm(range(params[\"num_epochs\"])):\n",
    "        epoch(train_dataloader, train_fn)\n",
    "\n",
    "    # Evaluation:\n",
    "    accuracy = test(test_dataloader, eval_fn)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Custom initialization\n",
    "Until now, we have been using `pxu.train` and `pxu.eval` to perform automatic initialization of the node values. However, this can also be performed manually for more flexibility. Here, we will initialize the nodes with the class average of the values computed for the previous batch. Note that the evaluation will still be based on forward initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxu.jit()\n",
    "def train_batch_avg_init(x, y, nodes_avg_by_class, *, T, model, optim_w, optim_x, n):\n",
    "    # We specify the node to which we want to forward to (`n`)\n",
    "    with pxu.train(model, x, forward_to=n):\n",
    "        # Initialize nodes\n",
    "        if nodes_avg_by_class is not None:\n",
    "            # We used the previously saved node averages to initialize the nodes from `n` to -1\n",
    "            # This syntax can be used only because we know that the number of samples per class is always the same\n",
    "            for node, avg_by_class in zip(model.nodes[n:-1], nodes_avg_by_class):\n",
    "                node[\"x\"] = jnp.repeat(avg_by_class, node[\"x\"].shape[0] // model.nm_classes, axis=0)\n",
    "\n",
    "        # Convert y to one_hot\n",
    "        y = jax.nn.one_hot(y, 10)\n",
    "\n",
    "        # Set last node to y\n",
    "        model.nodes[-1][\"x\"] = y\n",
    "\n",
    "        for i in range(T):\n",
    "            with pxu.step(model):\n",
    "                g, (v,) = train_x(x, y, model=model)\n",
    "                optim_x(g)\n",
    "\n",
    "        with pxu.step(model):\n",
    "            g, (v,) = train_w(x, y, model=model)\n",
    "            optim_w(g)\n",
    "\n",
    "    nodes_avg_by_class = [\n",
    "        # This syntax can be used only because we know that the number of samples per class is always the same\n",
    "        jnp.mean(jnp.reshape(node[\"x\"], (model.nm_classes, -1, *node[\"x\"].shape[1:])), axis=1)\n",
    "        for node in model.nodes[n:-1]\n",
    "    ]\n",
    "\n",
    "    return nodes_avg_by_class\n",
    "\n",
    "\n",
    "def epoch_avg_init(dl, train_fn, nodes_avg_by_class=None):\n",
    "    for batch in dl:\n",
    "        x, y = batch\n",
    "        nodes_avg_by_class = train_fn(x, y, nodes_avg_by_class)\n",
    "\n",
    "    return nodes_avg_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:16<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.13%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = Model(28 * 28, params[\"hidden_dim\"], 10, params[\"num_layers\"], jax.nn.gelu)\n",
    "    \n",
    "    # Create optimizers\n",
    "    with pxu.train(model, jnp.zeros((params[\"batch_size\"], 28 * 28)), None):\n",
    "        optim_x = pxu.Optim(\n",
    "            optax.sgd(params[\"x_learning_rate\"]),\n",
    "            model.parameters().filter(px.f(px.NodeParam)(frozen=False))\n",
    "        )\n",
    "        optim_w = pxu.Optim(\n",
    "            optax.adamw(params[\"w_learning_rate\"]),\n",
    "            model.parameters().filter(px.f(px.LayerParam)),\n",
    "        )\n",
    "\n",
    "    # Create snapshots\n",
    "    train_fn_avg_init = train_batch_avg_init.snapshot(T=params[\"T\"], model=model, optim_x=optim_x, optim_w=optim_w, n=2)\n",
    "    eval_fn = eval_batch.snapshot(model=model)\n",
    "\n",
    "    # Train:\n",
    "    nodes_avg_by_class = None\n",
    "    for e in tqdm(range(params[\"num_epochs\"])):\n",
    "        nodes_avg_by_class = epoch_avg_init(train_dataloader, train_fn_avg_init, nodes_avg_by_class)\n",
    "\n",
    "    # Evaluation:\n",
    "    accuracy = test(test_dataloader, eval_fn)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
